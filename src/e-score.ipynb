{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KMubAwwP04G"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-XZm7sajbmM",
    "outputId": "bb0721c6-14f5-4d0f-9ee6-ba68d0383d58"
   },
   "outputs": [],
   "source": [
    "# Install requirements from requirements file\n",
    "%pip install -r ../requirements.txt\n",
    "\n",
    "# Alternative: Install requirements directly\n",
    "# %pip install blosum\n",
    "# %pip install Bio\n",
    "# %pip install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# %pip install protein-bert\n",
    "# %pip install biopython biotite\n",
    "# %pip install fair-esm\n",
    "# %pip install scipy\n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3BStrXj6JQ2",
    "outputId": "af5c2e52-88a3-485f-f382-e704171b2969"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Riley\\iCloudDrive\\Repositories\\e-score\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using device: cpu\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "import blosum as bl\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "from scipy import stats\n",
    "import torch\n",
    "import esm\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "# Importing models and embeddings retrieval methods\n",
    "from models import get_model\n",
    "from embeddings import get_fasta_embeddings, get_pair_embeddings, load_fasta\n",
    "\n",
    "# Retrieve the device (CPU or GPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Available models\n",
    "MODELS_LIST = [\"ProtT5\", \"ProtBert\", \"ProtAlbert\", \"ProtXLNet\", \"ESM1b\", \"ESM2\"]\n",
    "\n",
    "# Available alignment types\n",
    "ALIGNMENT_TYPES = [\"Global-regular\" , \"Global-end-gap-free\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs6JenWnHYHo"
   },
   "source": [
    "# Alignment Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTFhk7slsYA1"
   },
   "source": [
    "## Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "r5iNmd9o6Q3V"
   },
   "outputs": [],
   "source": [
    "def affine_global_dp(seq_1, seq_2, g_open, g_ext,\n",
    "                     scoring=\"ProtT5\", Model=None, Model_tokenizer=None):\n",
    "    \"\"\" Performs global alignment of two sequences using affine gap penalty dynamic programming algorithm \"\"\"\n",
    "    m, n = len(seq_1), len(seq_2)\n",
    "\n",
    "    # Initialize matrices\n",
    "    M = np.zeros([m + 1, n + 1])\n",
    "    M[0, 1:] = g_open + g_ext * np.arange(0, n, 1)\n",
    "    M[1:, 0] = g_open + g_ext * np.arange(0, m, 1)\n",
    "\n",
    "    # Copies to avoid Gotoh's error\n",
    "    L, U = np.copy(M), np.copy(M)\n",
    "    L[1:, 0] += g_open;\n",
    "    U[0, 1:] += g_open\n",
    "\n",
    "    # Tracer matrix\n",
    "    tracer = np.zeros([np.shape(M)[0], np.shape(M)[1], 7])\n",
    "  \n",
    "    # Get embeddings\n",
    "    emb1, emb2 = get_pair_embeddings(seq_1, seq_2, Model, Model_tokenizer, scoring)\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    # Dynamic programming algorithm\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            l_arr = np.array([M[i, j - 1] + g_open, L[i, j - 1] + g_ext])\n",
    "            L[i, j] = np.max(l_arr)\n",
    "            l_where = l_arr == np.max(l_arr)\n",
    "\n",
    "            u_arr = np.array([M[i - 1, j] + g_open, U[i - 1, j] + g_ext])\n",
    "            U[i, j] = np.max(u_arr)\n",
    "            u_where = u_arr == np.max(u_arr)\n",
    "\n",
    "            if scoring in MODELS_LIST:\n",
    "                sim = cos(torch.tensor(emb1[i - 1], dtype=torch.float32), \n",
    "                          torch.tensor(emb2[j - 1], dtype=torch.float32)).item()\n",
    "                m_arr = np.array([M[i - 1, j - 1] + sim, U[i, j], L[i, j]])\n",
    "\n",
    "            M[i, j] = np.max(m_arr)\n",
    "            m_where = m_arr == np.max(m_arr)\n",
    "\n",
    "            idx = np.hstack([m_where, u_where, l_where])\n",
    "            tracer[i, j, idx] = 1\n",
    "\n",
    "    # Traceback\n",
    "    alignment = traceback_g(tracer, seq_1, seq_2, affine=True)\n",
    "    alignment = list(set(map(tuple, alignment)))\n",
    "\n",
    "    return M, L, U, tracer, alignment\n",
    "\n",
    "\n",
    "def traceback_g(tracer, seq_1, seq_2, mat=None, affine=False, roadmap=0):\n",
    "    \"\"\" Performs traceback step of the sequence alignment algorithm - reconstructs aligned sequences from the tracer matrix \n",
    "    Based on Smith-Waterman algorithm\n",
    "    \"\"\"\n",
    "    # Get sequence lengths\n",
    "    m, n = len(seq_1), len(seq_2)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(list(seq_1), dtype='object')\n",
    "    y = np.array(list(seq_2), dtype='object')\n",
    "\n",
    "    # Set start location\n",
    "    st = [m + 1, n + 1]\n",
    "    st_lv = 0  # Start in midgard\n",
    "\n",
    "    while st[0] > 1 and st[1] > 1:\n",
    "        B = np.zeros([2, 2])  # define 2x2 box which specifies which way to move\n",
    "\n",
    "        Tr_size = 7 if affine else 3  # Size of tracer array\n",
    "        Tr = np.zeros([Tr_size])  # define a Tr_size x 1 Tr array (will store arrows at each step)\n",
    "\n",
    "        # Get tracer array\n",
    "        for i in range(Tr_size):\n",
    "            Tr[i] = np.copy(tracer[st[0] - 1, st[1] - 1, i])\n",
    "\n",
    "        # Bifurcations\n",
    "        levels = [[2, 0, 1], [4, 3], [6, 5]] if affine else [[2, 0, 1]]\n",
    "\n",
    "        for l in levels:\n",
    "            if np.sum(Tr[l]) > 1:\n",
    "                choose = np.where(Tr[l] == 1)[0]\n",
    "                Tr[l] = 0\n",
    "                r = np.random.choice(choose, 1)[0] if roadmap == 0 else choose[-1] if roadmap == 1 else choose[0]\n",
    "                if roadmap not in [0, 1, 2]:\n",
    "                    raise Exception(\"roadmap only accepts 0: random turning, 1: highroad, 2: lowroad\")\n",
    "                Tr[l[r]] = 1\n",
    "\n",
    "        # Diagonal\n",
    "        if Tr[0] == 1 and st_lv == 0:\n",
    "            B[0, 0] = 1\n",
    "\n",
    "        # End gaps\n",
    "        if Tr[1] == 1 and st_lv == 0:\n",
    "            if affine:\n",
    "                st_lv = 1  # level up\n",
    "            else:\n",
    "                B[0, 1] = 1\n",
    "\n",
    "        if Tr[2] == 1 and st_lv == 0:\n",
    "            if affine:\n",
    "                st_lv = 2  # level down\n",
    "            else:\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        # Affine gaps allow for level shifts\n",
    "        if affine:\n",
    "            if Tr[4] == 1 and st_lv == 1:  # move up\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if Tr[3] == 1 and st_lv == 1:  # move up back to main\n",
    "                st_lv = 0\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if Tr[6] == 1 and st_lv == 2:  # move left\n",
    "                B[1, 0] = 1\n",
    "\n",
    "            if Tr[5] == 1 and st_lv == 2:  # move left back to main\n",
    "                st_lv = 0\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        # Movements\n",
    "        if B[0, 1] == 1:  # upward\n",
    "            y = np.insert(y, st[1] - 1, '-')  # add a gap\n",
    "            st[0] -= 1\n",
    "\n",
    "        if B[1, 0] == 1:  # leftward\n",
    "            x = np.insert(x, st[0] - 1, '-')  # add a gap\n",
    "            st[1] -= 1\n",
    "\n",
    "        if B[0, 0] == 1:  # diagonal\n",
    "            st[1] -= 1\n",
    "            st[0] -= 1\n",
    "\n",
    "    # Some end gaps are left when you hit the upper/lower end of the matrix or a 0\n",
    "    end_size = np.size(x) - np.size(y)  # how many gaps and for which sequence\n",
    "    end_gap = ['-'] * abs(end_size)\n",
    "    if end_size > 0:\n",
    "        y = np.insert(y, 0, end_gap)\n",
    "    elif end_size < 0:\n",
    "        x = np.insert(x, 0, end_gap)\n",
    "\n",
    "    # Check no overlapping gaps\n",
    "    x = np.where(((x == '-') & (y == '-')), None, x)\n",
    "    y = np.where((x == None), '', y)\n",
    "    x = np.where((x == None), '', x)\n",
    "\n",
    "    return np.sum(x), np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiNZQ2SQJM54"
   },
   "source": [
    "## Prefix/Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MqzYFa7dJTY9"
   },
   "outputs": [],
   "source": [
    "def affine_semi_global_dp(seq_1, seq_2, g_open, g_ext,\n",
    "                          high_low=False, scoring=\"ProtT5\", Model=None, Model_tokenizer=None):\n",
    "    \"\"\" Performs semi-global alignment of two sequences using affine gap penalty dynamic programming algorithm \"\"\"\n",
    "    m, n = len(seq_1), len(seq_2)\n",
    "    \n",
    "    # Initialize matrices\n",
    "    M = np.zeros([m + 1, n + 1])\n",
    "    L, U = np.copy(M), np.copy(M)\n",
    "\n",
    "    # Tracer matrix\n",
    "    tracer = np.zeros([np.shape(M)[0], np.shape(M)[1], 7])\n",
    "\n",
    "    # Get embeddings\n",
    "    emb1, emb2 = get_pair_embeddings(seq_1, seq_2, Model, Model_tokenizer, scoring)\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    # Dynamic programming algorithm\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            l_arr = np.array([M[i, j - 1] + g_open, L[i, j - 1] + g_ext])\n",
    "            L[i, j] = np.max(l_arr)\n",
    "            l_where = l_arr == np.max(l_arr)\n",
    "\n",
    "            u_arr = np.array([M[i - 1, j] + g_open, U[i - 1, j] + g_ext])\n",
    "            U[i, j] = np.max(u_arr)\n",
    "            u_where = u_arr == np.max(u_arr)\n",
    "\n",
    "            if scoring in MODELS_LIST:\n",
    "                sim = cos(torch.tensor(emb1[i - 1], dtype=torch.float32), \n",
    "                          torch.tensor(emb2[j - 1], dtype=torch.float32)).item()\n",
    "                m_arr = np.array([M[i - 1, j - 1] + sim, U[i, j], L[i, j]])\n",
    "\n",
    "            M[i, j] = np.max(m_arr)\n",
    "            m_where = m_arr == np.max(m_arr)\n",
    "\n",
    "            idx = np.hstack([m_where, u_where, l_where])\n",
    "            tracer[i, j, idx] = 1\n",
    "\n",
    "    alignment = [traceback_sg(tracer, seq_1, seq_2, mat=M, affine=True, local= True, roadmap=0)]\n",
    "    alignment = list(set(map(tuple, alignment)))\n",
    "\n",
    "    return M, L, U, tracer, alignment\n",
    "\n",
    "\n",
    "def traceback_sg(tracer, seq_1, seq_2, mat=None, local=False, affine=False, roadmap=0):\n",
    "    \"\"\" Performs traceback step of the sequence alignment algorithm - reconstructs aligned sequences from the tracer matrix \"\"\"\n",
    "    m, n = len(seq_1), len(seq_2)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(list(seq_1), dtype='object')\n",
    "    y = np.array(list(seq_2), dtype='object')\n",
    "\n",
    "    # Set start location (random maxima if 0)\n",
    "    r = np.random.choice(range(np.size(np.where(mat == np.max(mat))[0])), 1)[0] if roadmap == 0 else -1 if roadmap == 1 else 0\n",
    "    st = [(np.where(mat == np.max(mat))[0][r]) + 1, (np.where(mat == np.max(mat))[1][r]) + 1]\n",
    "\n",
    "    # Set starting gaps based on the start location\n",
    "    start_size = (m - st[0]) - (n - st[1])  # how many gaps and for which sequence\n",
    "    start_gap = (['-'] * abs(start_size))\n",
    "    if start_size > 0:\n",
    "        y = np.append(y, start_gap)\n",
    "    elif start_size < 0:\n",
    "        x = np.append(x, start_gap)\n",
    "\n",
    "    st_lv = 0  # start in midgard\n",
    "\n",
    "    while st[0] > 1 and st[1] > 1:\n",
    "        B = np.zeros([2, 2])  # define 2x2 box which specifies which way to move\n",
    "\n",
    "        Tr_size = 7 if affine else 3  # size of tracer array\n",
    "        Tr = np.zeros([Tr_size])  # define a Tr_size x 1 Tr array (will store arrows at each step)\n",
    "\n",
    "        # Tracer\n",
    "        for i in range(Tr_size):\n",
    "            Tr[i] = np.copy(tracer[st[0] - 1, st[1] - 1, i])\n",
    "\n",
    "        # Bifurcations\n",
    "        levels = [[2, 0, 1], [4, 3], [6, 5]] if affine else [[2, 0, 1]]\n",
    "\n",
    "        for l in levels:\n",
    "            if np.sum(Tr[l]) > 1:\n",
    "                choose = np.where(Tr[l] == 1)[0]\n",
    "                Tr[l] = 0\n",
    "                # 0: random turning, 1: highroad, 2: lowroad\n",
    "                r = np.random.choice(choose, 1)[0] if roadmap == 0 else choose[-1] if roadmap == 1 else choose[0]\n",
    "                if roadmap not in [0, 1, 2]:\n",
    "                    raise Exception(\"roadmap only accepts 0: random turning, 1: highroad, 2: lowroad\")\n",
    "                \n",
    "                Tr[l[r]] = 1\n",
    "\n",
    "        # level up-down\n",
    "        if Tr[0] == 1 and st_lv == 0:  # diagonal\n",
    "            B[0, 0] = 1\n",
    "\n",
    "        if Tr[1] == 1 and st_lv == 0:\n",
    "            if affine:\n",
    "                st_lv = 1  # level up\n",
    "            else:\n",
    "                B[0, 1] = 1\n",
    "\n",
    "        if Tr[2] == 1 and st_lv == 0:\n",
    "            if affine:\n",
    "                st_lv = 2  # level down\n",
    "            else:\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        # Affine gaps allow for level shifts\n",
    "        if affine:\n",
    "            if Tr[4] == 1 and st_lv == 1:  # move up\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if Tr[3] == 1 and st_lv == 1:  # move up back to main\n",
    "                st_lv = 0\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if Tr[6] == 1 and st_lv == 2:  # move left\n",
    "                B[1, 0] = 1\n",
    "\n",
    "            if Tr[5] == 1 and st_lv == 2:  # move left back to main\n",
    "                st_lv = 0\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        if local and mat[st[0] - 1, st[1] - 1] == 0:\n",
    "            break\n",
    "\n",
    "        # Movements\n",
    "        if B[0, 1] == 1:  # upward\n",
    "            y = np.insert(y, st[1] - 1, '-')  # add a gap\n",
    "            st[0] -= 1\n",
    "\n",
    "        if B[1, 0] == 1:  # leftward\n",
    "            x = np.insert(x, st[0] - 1, '-')  # add a gap\n",
    "            st[1] -= 1\n",
    "\n",
    "        if B[0, 0] == 1:  # diagonal\n",
    "            st[1] -= 1\n",
    "            st[0] -= 1\n",
    "\n",
    "    # Some end gaps are left when you hit the upper/lower end of the matrix or a 0\n",
    "    end_size = np.size(x) - np.size(y)  # how many gaps and for which sequence\n",
    "    end_gap = (['-'] * abs(end_size))\n",
    "    if end_size > 0:\n",
    "        y = np.insert(y, 0, end_gap)\n",
    "    elif end_size < 0:\n",
    "        x = np.insert(x, 0, end_gap)\n",
    "\n",
    "    # Check no overlapping gaps\n",
    "    x = np.where(((x == '-') & (y == '-')), None, x)\n",
    "    y = np.where((x == None), '', y)\n",
    "    x = np.where((x == None), '', x)\n",
    "\n",
    "    return np.sum(x), np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IMkkTPWsaF0"
   },
   "source": [
    "# Aux Funx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "LBNOVyaN7ZT3"
   },
   "outputs": [],
   "source": [
    "def get_sequences_only(seqs):\n",
    "    \"\"\" Extracts a list of (sequence) from a list of (name, sequence) tuples \"\"\"\n",
    "    return [seq[1] for seq in seqs]\n",
    "\n",
    "def aligned_to_indexed(seqs):\n",
    "  \"\"\" Removes dashes (-) in a sequence and creates positions array for non-dash residues \"\"\"\n",
    "  # Replace dashes with empty string\n",
    "  no_dash = [seq.replace(\"-\", \"\") for seq in seqs]\n",
    "\n",
    "  # Create positions array for non-dash residues\n",
    "  positions = [[i for i, char in enumerate(seq) if char != \"-\"] for seq in seqs]\n",
    "\n",
    "  return no_dash, positions\n",
    "\n",
    "def length_matcher(x , y , place = \"\"):\n",
    "  \"\"\" Matches the length between x and y with spaces if necessary \"\"\"\n",
    "  length = 5\n",
    "\n",
    "  def add_spaces(value):\n",
    "    spaces = abs(len(value) - length)\n",
    "    return \" \" * spaces + value if place == \"Back\" else value + \" \" * spaces\n",
    "\n",
    "  return add_spaces(x), add_spaces(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "achF78kUscys"
   },
   "source": [
    "# Alignment Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DawofHCVAdVo"
   },
   "outputs": [],
   "source": [
    "def get_alignments(prot1, prot2, gap_penalty = 0, gap_extension_penalty = 0,\n",
    "                   scoring = \"ProtT5\", alignment_type = \"Global-regular\", Model = \"\", Model_Tokenizer = \"\"):\n",
    "  \"\"\" Gets the alignments between two sequences \"\"\"\n",
    "  \n",
    "  if alignment_type == \"Global-regular\":\n",
    "    M, L, U, tracer, alignment = affine_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty, \n",
    "                                                  scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
    "    max_score = np.max(M)\n",
    "\n",
    "  if alignment_type == \"Global-end-gap-free\" or alignment_type == \"End-Gap-Free\":\n",
    "    M, L, U, tracer, alignment = affine_semi_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty, \n",
    "                                                        scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
    "    max_score = max(M[-1,-1],L[-1,-1],U[-1,-1])\n",
    "\n",
    "  # Return (reference alignment, query alignment, alignment score)\n",
    "  aligned1 = alignment[0][0]\n",
    "  aligned2 = alignment[0][1]\n",
    "\n",
    "  return aligned1, aligned2, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5Hfj5rAk4_cg"
   },
   "outputs": [],
   "source": [
    "def get_visualization(prot1, prot2, score, Type = \"\", Model = \"\", Model_Tokenizer = \"\"):\n",
    "  MODELS_LIST = [\"ProtT5\" , \"ProtBert\" , \"ProtAlbert\" , \"ProtXLNet\" , \"ESM1b\" , \"ESM2\"]\n",
    "  cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "  seqs = [prot1 , prot2]\n",
    "  no_dash, positions = aligned_to_indexed(seqs)\n",
    "\n",
    "  # Get embeddings\n",
    "  p1_emb, p2_emb = get_pair_embeddings(no_dash[0], no_dash[1], Model, Model_Tokenizer, Type)\n",
    "\n",
    "  p1_revived = \"\"\n",
    "  p2_revived = \"\"\n",
    "  aligned_info = \"\"\n",
    "\n",
    "  for i in range(len(prot1)):\n",
    "    if i in positions[0]:\n",
    "      p1_revived += prot1[i]\n",
    "    else:\n",
    "      p1_revived += \"-\"\n",
    "\n",
    "    if i in positions[1]:\n",
    "      p2_revived += prot2[i]\n",
    "    else:\n",
    "      p2_revived += \"-\"\n",
    "\n",
    "\n",
    "    if p1_revived[-1] == p2_revived[-1]:\n",
    "      aligned_info += p1_revived[-1]\n",
    "    elif p1_revived[-1] == \"-\" or p2_revived[-1] == \"-\":\n",
    "      aligned_info += \" \"\n",
    "    elif p1_revived[-1] != p2_revived[-1]:\n",
    "      if Type in MODELS_LIST:\n",
    "        sim = cos(torch.tensor(p1_emb[0][positions[0].index(i)] , dtype = torch.float32) ,\n",
    "                  torch.tensor(p2_emb[0][positions[1].index(i)] , dtype = torch.float32)).item()\n",
    "        aligned_info += \" \"\n",
    "\n",
    "  del model\n",
    "  del tokenizer\n",
    "\n",
    "  return p1_revived, aligned_info, p2_revived, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7nB5_MdnBO5"
   },
   "source": [
    "# Alignment For 2 Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dyAQgxCFn99V"
   },
   "outputs": [],
   "source": [
    "def alignment_file_TXT(saving_add, seqs_path, scoring, alignment_type,\n",
    "                      gap_penalty, gap_extension_penalty):\n",
    "  \"\"\" Creates and outputs the alignment file between two sequences \"\"\"\n",
    "  print(f\"Device: {device}\")\n",
    "\n",
    "  # Get selected model\n",
    "  Model , Model_Tokenizer = get_model(scoring)\n",
    "\n",
    "  # Load sequences from FASTA file\n",
    "  seqs = load_fasta(seqs_path)\n",
    "\n",
    "  # Get protein sequences\n",
    "  prot1 = seqs[0][1]\n",
    "  prot2 = seqs[1][1]\n",
    "\n",
    "  # Get names of protein sequences\n",
    "  name1 = seqs[0][0]\n",
    "  name2 = seqs[1][0]\n",
    "\n",
    "  # Get alignments and visualization\n",
    "  reference_al, query_al, al_score = get_alignments(prot1, prot2, gap_penalty = gap_penalty,\n",
    "                                              gap_extension_penalty = gap_extension_penalty ,\n",
    "                                              scoring = scoring , alignment_type = alignment_type,\n",
    "                                              Model = Model , Model_Tokenizer = Model_Tokenizer)\n",
    "\n",
    "  p1_al , aligned_info , p2_al , al_score = get_visualization(reference_al , query_al, al_score , Type = scoring,\n",
    "                                                              Model = Model, Model_Tokenizer = Model_Tokenizer)\n",
    "\n",
    "  if not os.path.exists(saving_add):\n",
    "    os.makedirs(saving_add)\n",
    "\n",
    "  file_name = saving_add + seqs_path.split(\"/\")[-1].split(\".\")[-2] + \"_\" + scoring + \"_\" + alignment_type + \"_\"\n",
    "  file_name += str(gap_penalty) + \"_\" + str(gap_extension_penalty) + \"_\"+ \"Alignment\" + \".txt\"\n",
    "  f = open(file_name, \"w\")\n",
    "\n",
    "  # Write Sequence 1 Information\n",
    "  f.write(\"Seq 1 \\n\")\n",
    "  f.write(\">\" + name1 + \"\\n\")\n",
    "  f.write(reference_al.replace(\"-\" , \"\") + \"\\n\")\n",
    "    \n",
    "  # Write Sequence 2 Information\n",
    "  f.write(\"Seq 2 \\n\")\n",
    "  f.write(\">\" + name2 + \"\\n\")\n",
    "  f.write(query_al.replace(\"-\" , \"\") + \"\\n\\n\")\n",
    "    \n",
    "  # Write Alignment Information\n",
    "  f.write(\"Alignment Type : \" + alignment_type + \"\\n\\n\")\n",
    "  f.write(\"Opening Gap Penalty : \" + str(gap_penalty) + \"\\n\")\n",
    "  f.write(\"Extension Gap Penalty : \" + str(gap_extension_penalty) + \"\\n\")\n",
    "  f.write(\"Scoring System : \" + scoring + \"\\n\")\n",
    "  f.write(\"Score : \"  + str(al_score) + \"\\n\\n\")\n",
    "\n",
    "  p1_pos = 1\n",
    "  p2_pos = 1\n",
    "  aligned_gaps = \"\"\n",
    "\n",
    "  for j in range(int(len(p1_al) / 60) + 1):\n",
    "    p1_posix = p1_al[j * 60: (j + 1) * 60]\n",
    "    p2_posix = p2_al[j * 60: (j + 1) * 60]\n",
    "    p1_back_str, p2_back_str = length_matcher(str(p1_pos) , str(p2_pos) , place = \"Front\")\n",
    "\n",
    "    for k in range(len(p1_posix)):\n",
    "      if p1_posix[k] != \"-\":\n",
    "        p1_pos += 1\n",
    "      if p2_posix[k] != \"-\":\n",
    "        p2_pos += 1\n",
    "\n",
    "    p1_end_str, p2_end_str = length_matcher(str(p1_pos - 1) , str(p2_pos - 1) , place = \"Back\")\n",
    "    aligned_gaps = \" \" * len(p1_back_str)\n",
    "\n",
    "    f.write(\"Seq 1 : \" + p1_back_str + \" \" + p1_al[j * 60: (j + 1) * 60] + \" \" + p1_end_str + \"\\n\")\n",
    "    f.write(\"        \"  +  aligned_gaps + \" \" + aligned_info[j * 60: (j + 1) * 60] + \"\\n\")\n",
    "    f.write(\"Seq 2 : \"  + p2_back_str + \" \" + p2_al[j * 60: (j + 1) * 60] + \" \" + p2_end_str + \"\\n\\n\")\n",
    "\n",
    "  print(\"Alignment Computation is Done!\")\n",
    "  del Model\n",
    "  del Model_Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1780860152.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    for j in range(n):\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def get_cosine_similarities(seq_1, seq_2, model, tokenizer, scoring):\n",
    "    \"\"\" Gets the cosine similarity between two sequences \"\"\"\n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    # Get embeddings\n",
    "    emb1, emb2 = get_pair_embeddings(seq_1, seq_2, model, tokenizer, scoring)\n",
    "\n",
    "    m, n = len(seq_1), len(seq_2)\n",
    "\n",
    "    cosine_similarities = np.zeros([m, n])\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            sim = cos(torch.tensor(emb1[i], dtype=torch.float32), \n",
    "                      torch.tensor(emb2[j], dtype=torch.float32)).item()\n",
    "            cosine_similarities[i, j] = sim\n",
    "            \n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OaiQRMPjkWbg"
   },
   "outputs": [],
   "source": [
    "def user_guide(MODELS_LIST):\n",
    "  \"\"\" User guide for the E-score program \"\"\"\n",
    "  print(\"Parameters & Descriptions:\")\n",
    "  print(\"  saving_add:\".ljust(25) + \"Output directory path\")\n",
    "  print(\"  seqs_path:\".ljust(25) + \"FASTA file with two protein sequences\")\n",
    "    \n",
    "  print(\"  scoring_type:\".ljust(25) + \"Model for embedding production (\", end = \"\")\n",
    "  for model_name in MODELS_LIST[:-1] : print(model_name + \", \" , end = \"\")\n",
    "  print(MODELS_LIST[-1] + \")\")\n",
    "\n",
    "  print(\"  alignment_type:\".ljust(25) + \"Global-regular or Global-end-gap-free\")\n",
    "  print(\"  gap_penalty:\".ljust(25) + \"Default: -1.0 | Recommended Values: -4.0, -3.0, -2.0, -1.5, -1.0, -0.5\")\n",
    "  print(\"  gap_extension_penalty:\".ljust(25) + \"Default: -0.2 | Recommended Values: -1.0, -0.8, -0.5, -0.3, -0.2, -0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVRUmBYkG4M3"
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Tyat_ydz5f4",
    "outputId": "d0b958ab-ed49-469e-ae38-44c0c744c10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters & Descriptions:\n",
      "  saving_add:            Output directory path\n",
      "  seqs_path:             FASTA file with two protein sequences\n",
      "  scoring_type:          Model for embedding production (ProtT5, ProtBert, ProtAlbert, ProtXLNet, ESM1b, ESM2)\n",
      "  alignment_type:        Global-regular or Global-end-gap-free\n",
      "  gap_penalty:           Default: -1.0 | Recommended Values: -4.0, -3.0, -2.0, -1.5, -1.0, -0.5\n",
      "  gap_extension_penalty: Default: -0.2 | Recommended Values: -1.0, -0.8, -0.5, -0.3, -0.2, -0.1\n"
     ]
    }
   ],
   "source": [
    "user_guide(MODELS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glng92tRG5ZZ",
    "outputId": "f876fd26-98b1-4520-c2ed-337e2758f0ff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Initializing ProtT5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating T5 Embeddings\n",
      "Generating T5 Embeddings\n",
      "Alignment Computation is Done!\n"
     ]
    }
   ],
   "source": [
    "# Defining Parameters\n",
    "saving_add =  \"./content/\"\n",
    "seqs_path = \"../data/Test2.fasta\"\n",
    "scoring = MODELS_LIST[0]\n",
    "alignment_type = ALIGNMENT_TYPES[0]\n",
    "gap_penalty = -1\n",
    "gap_extension_penalty = -0.2\n",
    "\n",
    "# Generating Alignment File\n",
    "# alignment_file_TXT(saving_add = saving_add , seqs_path = seqs_path, scoring = scoring, alignment_type = alignment_type, gap_penalty = gap_penalty, gap_extension_penalty = gap_extension_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_embeddings(seqs_path, model, tokenizer, scoring):\n",
    "    \"\"\" Analyze the embeddings between two sequences \"\"\"\n",
    "    embeddings = get_fasta_embeddings(seqs_path, model, tokenizer, scoring)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ProtT5\n",
      "Generating T5 Embeddings\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "saving_add = \"./analysis/\"\n",
    "seqs_path = \"../data/Test2.fasta\"\n",
    "scoring = MODELS_LIST[0] # ProtT5\n",
    "alignment_type = ALIGNMENT_TYPES[0] # Global\n",
    "\n",
    "Model, Tokenizer = get_model(scoring)\n",
    "\n",
    "# Analysis\n",
    "print(analyze_embeddings(seqs_path, Model, Tokenizer, scoring))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Dq9u86RyHKk_",
    "8LCdgRFBFWW6",
    "8ctdjBhTGi9z",
    "wiNZQ2SQJM54",
    "9IMkkTPWsaF0"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
