%---------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%---------------------------------------------
\documentclass[
	letterpaper, % Letter Paper (Required for 4490Z)
	10pt, % Font Size
	%twoside, % Commented out for fixed headers/footers
]{LTJournalArticle}

% BibLaTeX bibliography file
\addbibresource{sample.bib}

% Running Header
\runninghead{Gavigan, Riley}

% Footer Content
\footertext{}

% Page Counter Starting Number
\setcounter{page}{1}

%---------------------------------------------
%	TITLE SECTION
%---------------------------------------------
\title{Explaining embedding vector results for alignments\\ Thesis proposal (CS 4490Z)}

% Authors are listed in a comma-separated list with superscript numbers indicating affiliations
% \thanks{} is used for any text that should be placed in a footnote on the first page, such as the corresponding author's email, journal acceptance dates, a copyright/license notice, keywords, etc
\author{
	Riley Gavigan\textsuperscript{1}, Lucian Ilie\textsuperscript{1}\thanks{Thesis supervisor. Bioinformatics Lab - csd.uwo.ca/\textasciitilde ilie/lab.html}, Nazim Madhavji\textsuperscript{1}\thanks{Course supervisor}\\
}

% Affiliations are output in the \date{} command
\date{{\footnotesize{\textsuperscript{\textbf{1}}Department of Computer Science, University of Western Ontario, London, N6A 5B7, Ontario, Canada\\}}\vspace{1em}November 27, 2023\\}

% Full-width General Abstract
\renewcommand{\maketitlehookd}{%
	\begin{abstract}
		\noindent The proposed E-score alignment scoring method \autocite{Ashrafzadeh:2023} generates results that further research will explore, leading to insight for performance improvement. Cosine similarity results generated by the models were mostly positive instead of being randomly distributed. Exploration of this distribution will lead to a better understanding of the different models and the embedding vectors being produced. Different embedding models also differ greatly in performance with varying mean ranges. From this research, insight will be obtained to improve embedding performance and transformer model architecture.
	\end{abstract}
}
%------------------------------------------
\begin{document}
\maketitle % Output the title section

%---------------------------------------------
%	STRUCTURED ABSTRACT
%---------------------------------------------
\section{Structured Abstract}

% Context and Motivation
\subsection{Context and motivation}
The \textit{E}-score protein alignment scoring method \autocite{Ashrafzadeh:2023} outperforms state-of-the-art methods, supported by comparing ProtT5 \autocite{Elnaggar:2021} \textit{E}-score results with BLOSUM62 \autocite{Henikoff:1992}.

This research aims to understand \textit{E}-score results, building upon the observation that mean cosine similarity results between two embeddings are not evenly distributed.

By understanding the underlying causes of the observed results, we can improve the \textit{E}-score method. Insights can be used to fine-tune the transformer models \autocite{Elnaggar:2021, Rives:2021} and performance of embeddings.

% Research Questions
\subsection{Research questions}
\begin{itemize}
    \item{What properties of embeddings produce better cosine similarity results?}
    \item{Why does ProtT5 produce the best embeddings?}
    \item{Why do cosine similarity results primarily fall within a positive range?}
    \item{How can the underlying models be fine-tuned to produce better embeddings?}
\end{itemize}

% Principal Ideas
\subsection{Principal ideas}
Positive cosine similarity results imply the produced embeddings are mostly similar. Comparing different embedding types will provide insight into their distributions. Through these comparisons, conclusions about properties that improve \textit{E}-score results can be drawn.

% Research Methodology
\subsection{Research methodology}
This research is a data science investigation to obtain insight about the embeddings and cosine similarity results in the \textit{E}-score method.

\noindent Research resources:
\begin{itemize}[noitemsep]
    \item{\href{https://github.com/rgavigan/e-score}{\textit{E}-score analysis notebook}}
    \item{\href{https://github.com/agemagician/ProtTrans/tree/master/Fine-Tuning}{ProtT5 fine-tuning notebook}}
    \item{\href{https://aws.amazon.com/sagemaker/studio/}{SageMaker Studio} and \href{https://www.csd.uwo.ca/~ilie/lab.html}{Bioinformatics Lab} compute power}
\end{itemize}

% Anticipated Type of Results
\subsection{Anticipated type of results}
This study primarily aims to obtain insight and knowledge for the \textit{E}-score method, specifically:
\begin{itemize}
    \item{Knowledge about the distributions of different embedding types}
    \item{Knowledge about the cosine similarity between embeddings}
    \item{Insight to fine-tune and improve the available models}
    \item{Findings that can be reproduced for Natural Language Processing models and embeddings}
\end{itemize}

% Anticipated Novelty
\subsection{Anticipated novelty}
By building upon a novel method for scoring protein alignments using cosine similarity \autocite{Ashrafzadeh:2023}, anticipated results from this research will be novel conclusions that can further be explored and built upon.

% Anticipated Impact of Results
\subsection{Anticipated impact of results}
Improvements in transformer models for the \textit{E}-score alignment scoring method can be made through the insight this research finds. Any anticipated improvements would also be applicable to Natural Language Processing Models, such as T5 \autocite{Raffel:2020}.

%---------------------------------------------
%	BACKGROUND AND RELATED WORK
%---------------------------------------------
\section{Background and Related Work}

% Protein Transformers
\subsection{Protein transformers}
The \textit{E}-score alignment method depends on several protein transformer models to generate embeddings for a protein sequence prior to calculating cosine similarity. These transformers include ProtT5, ProtBert, ProtAlbert, and ProtXLNet\footnote{https://github.com/agemagician/ProtTrans} from the ProtTrans project \autocite{Elnaggar:2021}, as well as ESM1b and ESM2\footnote{https://github.com/facebookresearch/esm} from the Meta Fundamental AI Research Protein (FAIR) Team \autocite{Rives:2021}.



% Embedding Vectors
\subsection{Embedding vectors}


% E-Score Calculations
\subsection{\textit{E}-score calculations}

%---------------------------------------------
%	RESEARCH GOAL AND OBJECTIVES
%---------------------------------------------
\section{Research Goal and Objectives}
\textbf{Goal: }Obtain significant insight about the \textit{E}-score results and the differences between different models, leading to improvement of both the embedding performance and the embeddings being generated by the models.

Insight about performance differences between models has a significant impact outside of the \textit{E}-score method. These models are used extensively for other purposes and insight that can lead to performance improvements is important to the research community.

% Embedding Vector Distributions
\subsection{Embedding vector distributions}

By understanding the distributions of the embedding vector values, there will be a further understanding as to why the cosine similarity results are mostly positive (0..1) when calculating \textit{E}-score, instead of being randomly distributed from (-1..1).

Because cosine similarity averages are mostly positive, there are factors within the embedding vectors contributing to the angles between the embeddings being more similar than opposite.

% Cosine Similarity Results
\subsection{Cosine similarity results}

After having a stronger understanding of the embedding vector distributions, the cosine similarity results will be explored by visualizing the \textit{E}-score dispersion. Testing different combinations of alignments and modifying parameters will aid in exploring cosine similarity.

By understanding cosine similarity distributions, conclusions can be drawn about the factors contributing to the average results that were initially observed.

% Embedding Performance Differences
\subsection{Model performance differences}

The results between different embedding models vary greatly in their average range of values and in comparison to BLOSUM \autocite{Henikoff:1992} matrix results. Understanding why different embedding models generate different ranges in \textit{E}-score values will serve as a foundation for drawing significant research implications. 

With knowledge about the factors contributing to embedding performance, improvements can be made to the different models for scoring alignment.

%---------------------------------------------
%	RESEARCH APPROACH AND METHODOLOGY
%---------------------------------------------
\section{Research Approach and Methodology}

\subsection{Technical issues}

\subsection{Data}
Outline data to be used for testing and where to get it (protein sequences)

\subsection{Embedding vectors and cosine similarity}
Outline process of creating embedding vectors and getting cosine similarity for e-score

\subsection{Data analysis}
Outline algorithms, tools, python libraries/frameworks that will be used to visualize and analyze data and how it will be done.

% Heatmaps Figure
\begin{figure*} % Two column figure (notice the starred environment)
	\includegraphics[width=\linewidth]{Figures/Escorematrices.jpeg}
	\caption{Heatmaps of (a) BLOSUM62 matrix (scaled to -1..1) and three aligned matrices of average E-scores for the NBD\_sugar-kinase\_HSP70\_actin MSA: (b) ProtT5-score, (c) ProtAlbert-score, and (d) ProtXLNet-score.}
	\label{fig:escoreheatmap}
\end{figure*}

% Visualizing E-Score Dispersion
\subsubsection{Visualizing \textit{E}-score dispersion}

To gain a deeper understanding of the \textit{E}-score results generated for the different embeddings as shown in Figure \ref{fig:escoreheatmap}, dispersion will also be visualized. By visualizing the standard deviation and range of the results for each embedding type, there will be more insight to draw conclusions from. This will help understand why cosine similarity mean results are mostly positive (0..1), and why different models generate broader or narrower average ranges.

The \href{https://github.com/rgavigan/e-score}{\textit{E}-score analysis notebook} will contain all visualization calculations and results for \textit{E}-score dispersion.

% Comparing Embedding Models
\subsubsection{Comparing embedding models}

Average and dispersion \textit{E}-score results will be compared between the available embedding models to understand the factors contributing to the better performance of particular models over other models. This data will be used to draw conclusions about embedding performance differences.

% Modifying Parameters
\subsubsection{Modifying parameters}

By modifying different parameters and generating resulting \textit{E}-scores for different combinations from insight gained from differences between models, a greater understanding of the primary factors contributing to performance will provide insight to improve embeddings. Examples of parameters that can be modified are alignment type, gap penalty, and gap extension penalty.

\subsection{Interpretation}

\subsection{Type of results}

\subsection{Challenges or threats}

\subsection{Validation}

%---------------------------------------------
%	NOVELTY AND IMPACT
%---------------------------------------------
\section{Novelty and Impact}

The anticipated result from this research is thorough insight into the \textit{E}-score method's results. These data-supported findings are expected to contribute significantly to the improvement of embeddings and the models that generate them.

There are three primary advancements that can be explored from the implications drawn from the results. These would serve as beneficial follow-up research to improve embedding performance, transformer models, and to apply the same advancements to Natural Language Processing.

% Improving Embedding Perforamnce
\subsection{Improving embedding performance}

From the novel insight gained about the properties that contribute to the performance of different protein embeddings, the performance of the embeddings can be improved for sequence alignment. This would be an application of the data-supported explanation for why different embeddings have different cosine similarity averages and ranges.

% Transformer Model Improvement
\subsection{Transformer model improvement}

The above insights can be used as a starting point to modify the architecture of the different transformer \autocite{Vaswani:2017} models such as the ProtTrans models ProtT5, ProtBert, ProtXLNet, and ProtAlbert \autocite{Elnaggar:2021}. This research can be extended to improve these models through different processes such as hyperparameter tuning and optimization.

The ProtT5 model, the best performing model from the ProtTrans project, would serve as a valuable model to fine-tune with the research implications drawn from this research. The ProtTrans GitHub repository\footnote{https://github.com/agemagician/ProtTrans/tree/master/Fine-Tuning} provides Jupyter Notebooks for fine-tuning this model for per-protein prediction, per-residue classification, and per-residue regression.

% Natural Language Processing
\subsection{Natural Language Processing}
The ProtTrans models available for the \textit{E}-score method are all derivations of Natural Language Processing models modified to work with the language of proteins, the 20 amino acids.

All of the above research (improving embedding performance and transformer models) can be repeated for Natural Language Processing contextual embeddings such as ELMo \autocite{Peters:2018}, BERT \autocite{Devlin:2018}, RoBERTa \autocite{Liu:2019}, XLNet \autocite{Yang:2022}, and T5 \autocite{Raffel:2020}. 

%---------------------------------------------
%	REFERENCES
%---------------------------------------------
\printbibliography % Output the bibliography

%---------------------------------------------
\end{document}