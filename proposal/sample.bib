% Attention is all you need - Transformers
@ARTICLE{Vaswani:2017,
        author = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A.~N. and Kaiser, L. and Polosukhin, I.},
        title = {Attention is all you need},
        year = {2017},
        doi = {arXiv.1706.03762},
}

% ELMo
@ARTICLE{Peters:2018,
        author = {Peters, M.~E. and Neumann, M. and Iyyer, M. and Gardner, M. and Clark, C. and Lee, K. and Zettlemoyer, L.},
        title = {Deep contextualized word representations},
        year = {2018},
        doi = {arXiv:1802.05365},
}

% BERT
@ARTICLE{Devlin:2018,
        author = {Devlin, J. and Chang, M.~W. and Lee, K. and Toutanova, K.},
        title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
        year = {2018},
        doi = {arXiv:1810.04805},
}

% RoBERTa
@ARTICLE{Liu:2019,
        author = {Liu, Y. and Ott, M. and Goyal, N. and Du, J. and Joshi, M. and Chen, D. and Levy, O. and Lewis, M. and Zettlemoyer, L. and Stoyanov, V.},
        title = {Roberta: A robustly optimized bert pretraining approach},
        year = {2019},
        doi = {arXiv:1907.11692},
}

% XLNet
@ARTICLE{Yang:2022,
        author = {Yang, Z. and Dai, Z. and Yang, Y. and Carbonell, J. and Salakhutdinov, R.~R. and Le, Q.~V.},
        title = {XLNet: Generalized autoregressive pretraining for language understanding},
        journal = {Advances in neural information processing systems},
        year = {2019},
        doi = {10.48550/arXiv.1906.08237},
}

% T5
@ARTICLE{Raffel:2020,
        author = {Raffel, C. and Shazeer, N. and Roberts, A. and Lee, K. and Narang, S. and Matena, M. and Zhou, Y. and Li, W. and Liu, P.~J.},
        title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
        year = {2020},
        doi = {arXiv:1910.10683},
}

% ProtTrans
@ARTICLE{Elnaggar:2021,
        author={Elnaggar, A. and Heinzinger, M. and Dallago, C. and Rehawi, G. and Yu, W. and Jones, L. and Gibbs, T. and Feher, T. and Angerer, C. and Steinegger, M. and Bhowmik, D. and Rost, B.},
        journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
        title={ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing},
        year={2021},
        volume={},
        number={},
        pages={1-1},
        doi={10.1109/TPAMI.2021.3095381}
}

% BLOSUM
@ARTICLE{Henikoff:1992,
        author = {Henikoff, S. and Henikoff, J.~G.},
        title = {Amino acid substitution matrices from protein blocks},
        journal = {Proceedings of the National Academy of Sciences},
        year = {1992},
        doi = {10.1073/pnas.89.22.10915},
}

% ESM1b and ESM2
@article{Rives:2021,
  author={Rives, A. and Meier, J. and Sercu, T. and Goyal, S. and Lin, Z. and Liu, J. and Guo, D. and Ott, M. and Zitnick, C. L. and Ma, J. and Fergus, R.},
  title={Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences},
  year={2019},
  doi={10.1101/622803},
  url={https://www.biorxiv.org/content/10.1101/622803v4},
  journal={PNAS}
}

% E-Score Manuscript
@ARTICLE{Ashrafzadeh:2023,
        author = {Ashrafzadeh, S. and Brian Golding, G. and Ilie, L.},
        title = {Scoring alignments by embedding vector similarity},
        year = {2023},
        doi = {},
}