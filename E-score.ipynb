{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KMubAwwP04G"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-XZm7sajbmM",
    "outputId": "bb0721c6-14f5-4d0f-9ee6-ba68d0383d58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==1.4.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: accelerate==0.22.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (0.22.0)\n",
      "Requirement already satisfied: amqp==5.1.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.6.3)\n",
      "Requirement already satisfied: async-timeout==4.0.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (4.0.3)\n",
      "Requirement already satisfied: billiard==4.1.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (4.1.0)\n",
      "Requirement already satisfied: bio==1.5.9 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (1.5.9)\n",
      "Requirement already satisfied: biopython==1.81 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (1.81)\n",
      "Requirement already satisfied: biothings-client==0.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (0.3.0)\n",
      "Requirement already satisfied: biotite==0.37.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.37.0)\n",
      "Requirement already satisfied: blinker==1.6.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (1.6.2)\n",
      "Requirement already satisfied: cachetools==5.3.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (5.3.1)\n",
      "Requirement already satisfied: celery==5.3.4 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (5.3.4)\n",
      "Requirement already satisfied: certifi==2023.7.22 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer==3.2.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (3.2.0)\n",
      "Requirement already satisfied: click==8.1.7 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (8.1.7)\n",
      "Requirement already satisfied: click-didyoumean==0.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (0.3.0)\n",
      "Requirement already satisfied: click-plugins==1.1.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 18)) (1.1.1)\n",
      "Requirement already satisfied: click-repl==0.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 19)) (0.3.0)\n",
      "Requirement already satisfied: cmake==3.27.4.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 20)) (3.27.4.1)\n",
      "Requirement already satisfied: fair-esm==2.0.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 21)) (2.0.0)\n",
      "Requirement already satisfied: filelock==3.12.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (3.12.3)\n",
      "Requirement already satisfied: Flask==2.3.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 23)) (2.3.3)\n",
      "Requirement already satisfied: Flask-Mail==0.9.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 24)) (0.9.1)\n",
      "Requirement already satisfied: Flask-WTF==1.1.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (1.1.1)\n",
      "Requirement already satisfied: flatbuffers==23.5.26 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 26)) (23.5.26)\n",
      "Requirement already satisfied: fsspec==2023.9.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 27)) (2023.9.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (0.4.0)\n",
      "Requirement already satisfied: google-auth==2.22.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 29)) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib==1.0.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 30)) (1.0.0)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 31)) (0.2.0)\n",
      "Requirement already satisfied: gprofiler-official==1.0.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 32)) (1.0.0)\n",
      "Requirement already satisfied: grpcio==1.58.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 33)) (1.58.0)\n",
      "Requirement already satisfied: h5py==3.9.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub==0.16.4 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 35)) (0.16.4)\n",
      "Requirement already satisfied: idna==3.4 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 36)) (3.4)\n",
      "Requirement already satisfied: install==1.3.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 37)) (1.3.5)\n",
      "Requirement already satisfied: itsdangerous==2.1.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 38)) (2.1.2)\n",
      "Requirement already satisfied: jax==0.4.13 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 39)) (0.4.13)\n",
      "Requirement already satisfied: Jinja2==3.1.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 40)) (3.1.2)\n",
      "Requirement already satisfied: keras==2.12.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 41)) (2.12.0)\n",
      "Requirement already satisfied: kombu==5.3.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 42)) (5.3.2)\n",
      "Requirement already satisfied: libclang==16.0.6 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 43)) (16.0.6)\n",
      "Requirement already satisfied: lit==16.0.6 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 44)) (16.0.6)\n",
      "Requirement already satisfied: lxml==4.9.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 45)) (4.9.3)\n",
      "Requirement already satisfied: Markdown==3.4.4 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 46)) (3.4.4)\n",
      "Requirement already satisfied: MarkupSafe==2.1.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 47)) (2.1.3)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 48)) (0.2.0)\n",
      "Requirement already satisfied: mpmath==1.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 49)) (1.3.0)\n",
      "Requirement already satisfied: msgpack==1.0.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 50)) (1.0.5)\n",
      "Requirement already satisfied: mygene==3.2.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 51)) (3.2.2)\n",
      "Requirement already satisfied: networkx==3.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 52)) (3.1)\n",
      "Requirement already satisfied: numpy==1.23.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 53)) (1.23.5)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 54)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 55)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 56)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 57)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 58)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 59)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 60)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 61)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 62)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 63)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 64)) (11.7.91)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 65)) (3.2.2)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 66)) (3.3.0)\n",
      "Requirement already satisfied: packaging==23.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 67)) (23.1)\n",
      "Requirement already satisfied: pandas==2.0.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 68)) (2.0.3)\n",
      "Requirement already satisfied: Pillow==10.0.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 69)) (10.0.0)\n",
      "Requirement already satisfied: platformdirs==3.10.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 70)) (3.10.0)\n",
      "Requirement already satisfied: pooch==1.7.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 71)) (1.7.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.39 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 72)) (3.0.39)\n",
      "Requirement already satisfied: protein-bert==1.0.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 73)) (1.0.1)\n",
      "Requirement already satisfied: protobuf==4.24.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 74)) (4.24.3)\n",
      "Requirement already satisfied: psutil==5.9.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 75)) (5.9.5)\n",
      "Requirement already satisfied: pyasn1==0.5.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 76)) (0.5.0)\n",
      "Requirement already satisfied: pyasn1-modules==0.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 77)) (0.3.0)\n",
      "Requirement already satisfied: pyfaidx==0.7.2.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 78)) (0.7.2.1)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 79)) (2.8.2)\n",
      "Requirement already satisfied: pytz==2023.3.post1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 80)) (2023.3.post1)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 81)) (6.0.1)\n",
      "Requirement already satisfied: redis==4.6.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 82)) (4.6.0)\n",
      "Requirement already satisfied: regex==2023.8.8 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 83)) (2023.8.8)\n",
      "Requirement already satisfied: requests==2.31.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 84)) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 85)) (1.3.1)\n",
      "Requirement already satisfied: rsa==4.9 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 86)) (4.9)\n",
      "Requirement already satisfied: safetensors==0.3.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 87)) (0.3.3)\n",
      "Requirement already satisfied: scipy==1.10.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 88)) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 89)) (0.1.99)\n",
      "Requirement already satisfied: six==1.16.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 90)) (1.16.0)\n",
      "Requirement already satisfied: sympy==1.12 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 91)) (1.12)\n",
      "Requirement already satisfied: tensorboard==2.12.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 92)) (2.12.3)\n",
      "Requirement already satisfied: tensorboard-data-server==0.7.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 93)) (0.7.1)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 94)) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-addons==0.21.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 95)) (0.21.0)\n",
      "Requirement already satisfied: tensorflow-estimator==2.12.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 96)) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.34.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 97)) (0.34.0)\n",
      "Requirement already satisfied: termcolor==2.3.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 98)) (2.3.0)\n",
      "Requirement already satisfied: tokenizers==0.13.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 99)) (0.13.3)\n",
      "Requirement already satisfied: torch==2.0.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 100)) (2.0.1)\n",
      "Requirement already satisfied: torchaudio==2.0.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 101)) (2.0.2)\n",
      "Requirement already satisfied: torchvision==0.15.2 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 102)) (0.15.2)\n",
      "Requirement already satisfied: tqdm==4.66.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 103)) (4.66.1)\n",
      "Requirement already satisfied: transformers==4.33.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 104)) (4.33.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 105)) (2.0.0)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 106)) (2.13.3)\n",
      "Requirement already satisfied: typing_extensions==4.7.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 107)) (4.7.1)\n",
      "Requirement already satisfied: tzdata==2023.3 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 108)) (2023.3)\n",
      "Requirement already satisfied: urllib3==1.26.16 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 109)) (1.26.16)\n",
      "Requirement already satisfied: vine==5.0.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 110)) (5.0.0)\n",
      "Requirement already satisfied: wcwidth==0.2.6 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 111)) (0.2.6)\n",
      "Requirement already satisfied: Werkzeug==2.3.7 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 112)) (2.3.7)\n",
      "Requirement already satisfied: wrapt==1.14.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 113)) (1.14.1)\n",
      "Requirement already satisfied: WTForms==3.0.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from -r requirements.txt (line 114)) (3.0.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from astunparse==1.6.3->-r requirements.txt (line 4)) (0.41.3)\n",
      "Requirement already satisfied: backports.zoneinfo>=0.2.1 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from celery==5.3.4->-r requirements.txt (line 13)) (0.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from Flask==2.3.3->-r requirements.txt (line 23)) (6.8.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->-r requirements.txt (line 54)) (68.2.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/sagemaker-distribution/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->Flask==2.3.3->-r requirements.txt (line 23)) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install requirements from requirements file\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Alternative: Install requirements directly\n",
    "# !pip install blosum\n",
    "# !pip install Bio\n",
    "# !pip install torch torchvision torchaudio transformers sentencepiece accelerate --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "# !pip install protein-bert\n",
    "# !pip install biopython biotite\n",
    "# !pip install fair-esm\n",
    "# !pip install scipy\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3BStrXj6JQ2",
    "outputId": "af5c2e52-88a3-485f-f382-e704171b2969"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 18:08:34.184110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-06 18:08:35.017575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "import blosum as bl\n",
    "from Bio import SeqIO\n",
    "import random\n",
    "from scipy import stats\n",
    "import torch\n",
    "import esm\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import pickle\n",
    "import statistics\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "from transformers import AlbertModel, AlbertTokenizer\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import XLNetModel, XLNetTokenizer\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Retrieve the device (CPU or GPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device: {}\".format(device))\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Available models\n",
    "MODELS_LIST = [\"ProtT5\", \"ProtBert\", \"ProtAlbert\", \"ProtXLNet\", \"ESM1b\", \"ESM2\"]\n",
    "\n",
    "# Available alignment types\n",
    "ALIGNMENT_TYPES = [\"Global-regular\" , \"Global-end-gap-free\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dq9u86RyHKk_",
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTuu_8jcuDq_"
   },
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7VGs9HFct_B9"
   },
   "outputs": [],
   "source": [
    "def ProtT5_initialize():\n",
    "  print(\"Initializing ProtT5\")\n",
    "  transformer_link = \"Rostlab/prot_t5_xl_half_uniref50-enc\"\n",
    "    \n",
    "  print(\"Loading T5 Model: {}\".format(transformer_link))\n",
    "  T5 = T5EncoderModel.from_pretrained(transformer_link).to(device)\n",
    "    \n",
    "  # Run full-precision if using CPU, half-precision if GPU\n",
    "  T5.float() if device == torch.device('cpu') else T5.half()\n",
    "    \n",
    "  # Set to evaluation model\n",
    "  T5 = T5.eval()\n",
    "    \n",
    "  print(\"Loading T5 Tokenizer\")\n",
    "  T5_tokenizer = T5Tokenizer.from_pretrained(transformer_link, do_lower_case=False)\n",
    "\n",
    "  return T5 , T5_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HFSxYPYZuCs1"
   },
   "outputs": [],
   "source": [
    "def get_embs_T5(T5, tokenizer, sequences, n):\n",
    "  sequence_examples = sequences[:n]\n",
    "\n",
    "  # Replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "  sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "  # Tokenize sequences and pad up to the longest sequence in the batch\n",
    "  ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding= True)\n",
    "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "  # Generate embeddings\n",
    "  print(\"Generating T5 Embeddings\")\n",
    "  with torch.no_grad():\n",
    "      embedding_repr = T5(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "  last_layer_repr = embedding_repr.last_hidden_state\n",
    "  final_embs = []\n",
    "  for i in range(len(last_layer_repr)):\n",
    "    final_embs.append(last_layer_repr[i , :len(sequences[i])])\n",
    "\n",
    "  return final_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtP4O4na_FAe"
   },
   "source": [
    "## ESM1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "r6o9K45CGpxw"
   },
   "outputs": [],
   "source": [
    "def ESM1b_initialize():\n",
    "  # Load ESM-1b model\n",
    "  print(\"Initializing ESM1b\")\n",
    "  ESM1b, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  ESM1b.eval()  # disables dropout for deterministic results\n",
    "\n",
    "  return ESM1b, batch_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5VYaYR2lGuRj"
   },
   "outputs": [],
   "source": [
    "def get_embs_ESM1b(ESM1b, batch_converter, sequences, n):\n",
    "  sequences = sequences[:n]\n",
    "  data = [(\"\" , sequences[0])]\n",
    "\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "  # Extract per-residue representations\n",
    "  with torch.no_grad():\n",
    "      results = ESM1b(batch_tokens, repr_layers=[33], return_contacts= False)\n",
    "  token_representations = results[\"representations\"][33]\n",
    "\n",
    "  final_embs = []\n",
    "  for i in range(len(token_representations)):\n",
    "    final_embs.append(token_representations[i][1:-1])\n",
    "\n",
    "  return final_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4f4hYnn_J-Y"
   },
   "source": [
    "## ESM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0-hKY_uJ_RQK"
   },
   "outputs": [],
   "source": [
    "def ESM2_initialize():\n",
    "  print(\"Initializing ESM2\")\n",
    "  ESM2, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
    "  batch_converter = alphabet.get_batch_converter()\n",
    "  ESM2.eval()  # disables dropout for deterministic results\n",
    "\n",
    "  return ESM2, batch_converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3uy6XUL_BgXP"
   },
   "outputs": [],
   "source": [
    "def get_embs_ESM2(ESM2, batch_converter, sequences, n):\n",
    "  sequences = sequences[:n]\n",
    "  data = [(\"\" , sequences[0])]\n",
    "\n",
    "  batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "\n",
    "  # Extract per-residue representations\n",
    "  with torch.no_grad():\n",
    "      results = ESM2(batch_tokens, repr_layers=[33], return_contacts= False)\n",
    "  token_representations = results[\"representations\"][33]\n",
    "\n",
    "  final_embs = []\n",
    "  for i in range(len(token_representations)):\n",
    "    final_embs.append(token_representations[i][1:-1])\n",
    "\n",
    "  return final_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B0xNnG7IFLuU"
   },
   "source": [
    "## Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yIOMmp3nFR66"
   },
   "outputs": [],
   "source": [
    "def ProtBert_initialize():\n",
    "  print(\"Initializing ProtBert\")\n",
    "  transformer_link = \"Rostlab/prot_bert\"\n",
    "\n",
    "  Bert_tokenizer = BertTokenizer.from_pretrained(transformer_link, do_lower_case=False)\n",
    "  Bert = BertModel.from_pretrained(transformer_link)\n",
    "\n",
    "  Albert = Bert.to(device)\n",
    "  Albert = Bert.eval()\n",
    "\n",
    "  return Bert, Bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yzg4i4vqFSb6"
   },
   "outputs": [],
   "source": [
    "def get_embs_ProtBert(Bert , tokenizer , sequences , n):\n",
    "  sequence_examples = sequences[:n]\n",
    "\n",
    "  # this will replace all rare/ambiguous amino acids by X and introduce white-space between all amino acids\n",
    "  sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in sequence_examples]\n",
    "\n",
    "  ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, pad_to_max_length=True)\n",
    "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    embedding = Bert(input_ids=input_ids,attention_mask=attention_mask)[0]\n",
    "\n",
    "  final_embs = []\n",
    "  for seq_num in range(len(embedding)):\n",
    "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "      seq_emd = embedding[seq_num][1:seq_len-1]\n",
    "      final_embs.append(seq_emd)\n",
    "\n",
    "  return final_embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LCdgRFBFWW6"
   },
   "source": [
    "## Albert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tQE-EIVnGRNP"
   },
   "outputs": [],
   "source": [
    "def ProtAlbert_initialize():\n",
    "  print(\"Initializing ProtAlbert\")\n",
    "  transformer_link = \"Rostlab/prot_albert\"\n",
    "\n",
    "  Albert_tokenizer = AlbertTokenizer.from_pretrained(transformer_link, do_lower_case=False)\n",
    "  Albert = AlbertModel.from_pretrained(transformer_link)\n",
    "\n",
    "  Albert = Albert.to(device)\n",
    "  Albert = Albert.eval()\n",
    "\n",
    "  return Albert, Albert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QKjAfFMBGKQq"
   },
   "outputs": [],
   "source": [
    "def get_embs_ProtAlbert(Albert, Albert_tokenizer, sequences, n):\n",
    "\n",
    "  sequences = [\" \".join(re.sub(r\"[UZOB]\", \"X\", sequence)) for sequence in sequences]\n",
    "  ids = Albert_tokenizer.batch_encode_plus(sequences, add_special_tokens=True, padding = 'longest')\n",
    "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      embedding = Albert(input_ids = input_ids , attention_mask = attention_mask)[0]\n",
    "\n",
    "  features = []\n",
    "  for seq_num in range(len(embedding)):\n",
    "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "      seq_emd = embedding[seq_num][1 : seq_len - 1]\n",
    "      features.append(seq_emd)\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ctdjBhTGi9z"
   },
   "source": [
    "## XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_M2uk9EsGn00"
   },
   "outputs": [],
   "source": [
    "def XLNet_initialize():\n",
    "  print(\"Initializing ProtXLNet\")\n",
    "  transformer_link = \"Rostlab/prot_xlnet\"\n",
    "\n",
    "  XLNet_tokenizer = XLNetTokenizer.from_pretrained(transformer_link, do_lower_case=False)\n",
    "  XLNet = XLNetModel.from_pretrained(transformer_link, mem_len= 1024)\n",
    "\n",
    "  XLNet = XLNet.to(device)\n",
    "  XLNet = XLNet.eval()\n",
    "\n",
    "  return XLNet, XLNet_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bcwC7KpxGrCr"
   },
   "outputs": [],
   "source": [
    "def get_embs_XLNet(XLNet, XLNet_tokenizer, sequences, n):\n",
    "\n",
    "  sequences = [\" \".join(re.sub(r\"[UZOBX]\" , \"<unk>\", sequence)) for sequence in sequences]\n",
    "  ids = XLNet_tokenizer.batch_encode_plus(sequences, add_special_tokens = True, padding = 'longest')\n",
    "  input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "  attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "  with torch.no_grad():\n",
    "      output = XLNet(input_ids = input_ids , attention_mask = attention_mask)\n",
    "      embedding = output.last_hidden_state\n",
    "\n",
    "  features = []\n",
    "  for seq_num in range(len(embedding)):\n",
    "      seq_len = (attention_mask[seq_num] == 1).sum()\n",
    "      padded_seq_len = len(attention_mask[seq_num])\n",
    "      seq_emd = embedding[seq_num][padded_seq_len - seq_len : padded_seq_len - 2]\n",
    "      features.append(seq_emd)\n",
    "\n",
    "\n",
    "  return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs6JenWnHYHo"
   },
   "source": [
    "# Alignment Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTFhk7slsYA1"
   },
   "source": [
    "## Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "r5iNmd9o6Q3V"
   },
   "outputs": [],
   "source": [
    "def affine_global_dp(seq_1, seq_2, g_open, g_ext,\n",
    "                     scoring=\"ProtT5\", Model=None, Model_tokenizer=None):\n",
    "    # initialize the matrix\n",
    "    m = len(seq_1);\n",
    "    n = len(seq_2)\n",
    "    M = np.zeros([m + 1, n + 1])\n",
    "    M[0, 1:] = g_open + g_ext * np.arange(0, n, 1)\n",
    "    M[1:, 0] = g_open + g_ext * np.arange(0, m, 1)\n",
    "    L = np.copy(M);\n",
    "    U = np.copy(M)\n",
    "    L[1:, 0] = L[1:, 0] + g_open;\n",
    "    U[0, 1:] = U[0, 1:] + g_open  # avoiding Gotoh's error\n",
    "\n",
    "    # fill up\n",
    "    tracer = np.zeros([np.shape(M)[0], np.shape(M)[1], 7])\n",
    "\n",
    "    if scoring == \"ProtT5\":\n",
    "        emb1 = get_embs_T5(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_T5(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    if scoring == \"ProtBert\":\n",
    "        emb1 = get_embs_ProtBert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ProtBert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    if scoring == \"ProtAlbert\":\n",
    "        emb1 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    if scoring == \"ProtXLNet\":\n",
    "        emb1 = get_embs_XLNet(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_XLNet(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    if scoring == \"ESM1b\":\n",
    "        emb1 = get_embs_ESM1b(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ESM1b(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    if scoring == \"ESM2\":\n",
    "        emb1 = get_embs_ESM2(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ESM2(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            l_arr = np.array([M[i, j - 1] + g_open, L[i, j - 1] + g_ext])\n",
    "            L[i, j] = np.max(l_arr)\n",
    "            l_where = l_arr == np.max(l_arr)\n",
    "\n",
    "            u_arr = np.array([M[i - 1, j] + g_open, U[i - 1, j] + g_ext])\n",
    "            U[i, j] = np.max(u_arr)\n",
    "            u_where = u_arr == np.max(u_arr)\n",
    "\n",
    "            if scoring in MODELS_LIST:\n",
    "                sim = cos(torch.tensor(emb1[i - 1], dtype=torch.float32)\n",
    "                          , torch.tensor(emb2[j - 1], dtype=torch.float32)).item()\n",
    "\n",
    "                m_arr = np.array([M[i - 1, j - 1] + sim, U[i, j], L[i, j]])\n",
    "\n",
    "            M[i, j] = np.max(m_arr)\n",
    "            m_where = m_arr == np.max(m_arr)\n",
    "\n",
    "            idx = np.hstack([m_where, u_where, l_where])\n",
    "            tracer[i, j, idx] = 1\n",
    "\n",
    "    # traceback\n",
    "\n",
    "    alignment = []\n",
    "    alignment.append(traceback_g(tracer, seq_1, seq_2, affine= True, roadmap=0))\n",
    "\n",
    "    alignment = list(set(map(tuple, alignment)))\n",
    "\n",
    "    return M, L, U, tracer, alignment\n",
    "\n",
    "\n",
    "def traceback_g(tracer, seq_1, seq_2, mat=None, affine=False, roadmap=0):\n",
    "    # get sequence lengths\n",
    "    m = len(seq_1);\n",
    "    n = len(seq_2)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    x = np.array(list(seq_1), dtype='object')\n",
    "    y = np.array(list(seq_2), dtype='object')\n",
    "\n",
    "    # set start location\n",
    "    st = [m + 1, n + 1]\n",
    "\n",
    "    st_lv = 0  # start in midgard\n",
    "\n",
    "    while ((st[0] > 1) & (st[1] > 1)):\n",
    "\n",
    "        B = np.zeros([2, 2])  # define 2x2 box which specifies which way to move\n",
    "\n",
    "        if affine is True:\n",
    "            Tr = np.zeros([7])  # define a 7x1 Tr array (will store arrows at each step)\n",
    "        else:\n",
    "            Tr = np.zeros([3])  # define a 3x1 Tr array (will store arrows at each step)\n",
    "\n",
    "        if affine is False:\n",
    "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
    "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
    "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
    "\n",
    "        else:\n",
    "            # tracer\n",
    "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
    "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
    "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
    "            Tr[3] = np.copy(tracer[st[0] - 1, st[1] - 1, 3])\n",
    "            Tr[4] = np.copy(tracer[st[0] - 1, st[1] - 1, 4])\n",
    "            Tr[5] = np.copy(tracer[st[0] - 1, st[1] - 1, 5])\n",
    "            Tr[6] = np.copy(tracer[st[0] - 1, st[1] - 1, 6])\n",
    "\n",
    "        # bifurcations\n",
    "        if affine is True:\n",
    "            levels = [[2, 0, 1], [4, 3], [6, 5]]\n",
    "        else:\n",
    "            levels = [[2, 0, 1]]\n",
    "        for l in levels:\n",
    "            if np.sum(Tr[l]) > 1:\n",
    "                choose = np.where(Tr[l] == 1)[0]\n",
    "                Tr[l] = 0\n",
    "                if roadmap == 0:\n",
    "                    r = np.random.choice(choose, 1)[0]  # random turning\n",
    "                elif roadmap == 1:\n",
    "                    r = choose[-1]  # highroad\n",
    "                elif roadmap == 2:\n",
    "                    r = choose[0]  # lowroad\n",
    "                else:\n",
    "                    raise Exception(\"roadmap only accepts 0: random turning, 1: highroad, 2: lowroad\")\n",
    "                Tr[l[r]] = 1\n",
    "\n",
    "        # level up-down\n",
    "        if ((Tr[0] == 1) & (st_lv == 0)):  # diagonal\n",
    "            B[0, 0] = 1\n",
    "\n",
    "        if ((Tr[1] == 1) & (st_lv == 0)):\n",
    "            if affine is True:\n",
    "                st_lv = 1  # level up\n",
    "            else:\n",
    "                B[0, 1] = 1\n",
    "\n",
    "        if ((Tr[2] == 1) & (st_lv == 0)):\n",
    "            if affine is True:\n",
    "                st_lv = 2  # level down\n",
    "            else:\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        # affine gaps allow for level shifts\n",
    "        if affine is True:\n",
    "            if ((Tr[4] == 1) & (st_lv == 1)):  # move up\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if ((Tr[3] == 1) & (st_lv == 1)):  # move up back to main\n",
    "                st_lv = 0\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if ((Tr[6] == 1) & (st_lv == 2)):  # move left\n",
    "                B[1, 0] = 1\n",
    "\n",
    "            if ((Tr[5] == 1) & (st_lv == 2)):  # move left back to main\n",
    "                st_lv = 0\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        # movements\n",
    "        if B[0, 1] == 1:  # upward\n",
    "            y = np.insert(y, st[1] - 1, '-')  # add a gap\n",
    "            st[0] = st[0] - 1\n",
    "\n",
    "        if B[1, 0] == 1:  # leftward\n",
    "            x = np.insert(x, st[0] - 1, '-')  # add a gap\n",
    "            st[1] = st[1] - 1\n",
    "\n",
    "        if B[0, 0] == 1:  # diagonal\n",
    "            st[1] = st[1] - 1\n",
    "            st[0] = st[0] - 1\n",
    "\n",
    "    # some end gaps are left when you hit the upper/lower end of the matrix or a 0\n",
    "    end_size = (np.size(x) - np.size(y))  # how many gaps and for which sequence\n",
    "    end_gap = (['-'] * abs(end_size))\n",
    "    if end_size > 0:\n",
    "        y = np.insert(y, 0, end_gap)\n",
    "    elif end_size < 0:\n",
    "        x = np.insert(x, 0, end_gap)\n",
    "\n",
    "    # check no overlapping gaps\n",
    "    x = np.where(((x == '-') & (y == '-')), None, x)\n",
    "    y = np.where((x == None), '', y)\n",
    "    x = np.where((x == None), '', x)\n",
    "\n",
    "    return np.sum(x), np.sum(y)\n",
    "\n",
    "\n",
    "def traceback_iterator_g(tracer, seq_1, seq_2,\n",
    "                         affine=False):\n",
    "    alignment = []\n",
    "    alignment.append(traceback_g(tracer, seq_1, seq_2, affine=affine, roadmap=0))\n",
    "\n",
    "    return list(set(map(tuple, alignment)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiNZQ2SQJM54"
   },
   "source": [
    "## Prefix/Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MqzYFa7dJTY9"
   },
   "outputs": [],
   "source": [
    "def affine_semi_global_dp(seq_1, seq_2, g_open, g_ext,\n",
    "                          high_low=False, scoring=\"ProtT5\", Model=None, Model_tokenizer=None):\n",
    "    # Initialize the matrix\n",
    "    m = len(seq_1);\n",
    "    n = len(seq_2)\n",
    "    M = np.zeros([m + 1, n + 1])\n",
    "    M[0, 1:] = 0\n",
    "    M[1:, 0] = 0\n",
    "    L = np.copy(M);\n",
    "    U = np.copy(M)\n",
    "    L[1:, 0] = 0;\n",
    "    U[0, 1:] = 0\n",
    "\n",
    "    # Fill up\n",
    "    tracer = np.zeros([np.shape(M)[0], np.shape(M)[1], 7])\n",
    "\n",
    "    if scoring == \"ProtT5\":\n",
    "        emb1 = get_embs_T5(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_T5(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "        \n",
    "    elif scoring == \"ProtBert\":\n",
    "        emb1 = get_embs_ProtBert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ProtBert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    elif scoring == \"ProtAlbert\":\n",
    "        emb1 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ProtAlbert(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    elif scoring == \"ProtXLNet\":\n",
    "        emb1 = get_embs_XLNet(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_XLNet(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    elif scoring == \"ESM1b\":\n",
    "        emb1 = get_embs_ESM1b(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ESM1b(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    elif scoring == \"ESM2\":\n",
    "        emb1 = get_embs_ESM2(Model, Model_tokenizer, [seq_1], 1)[0].cpu().numpy()\n",
    "        emb2 = get_embs_ESM2(Model, Model_tokenizer, [seq_2], 1)[0].cpu().numpy()\n",
    "        cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            l_arr = np.array([M[i, j - 1] + g_open, L[i, j - 1] + g_ext])\n",
    "            L[i, j] = np.max(l_arr)\n",
    "            l_where = l_arr == np.max(l_arr)\n",
    "\n",
    "            u_arr = np.array([M[i - 1, j] + g_open, U[i - 1, j] + g_ext])\n",
    "            U[i, j] = np.max(u_arr)\n",
    "            u_where = u_arr == np.max(u_arr)\n",
    "\n",
    "            if scoring in MODELS_LIST:\n",
    "                sim = cos(torch.tensor(emb1[i - 1], dtype=torch.float32)\n",
    "                          , torch.tensor(emb2[j - 1], dtype=torch.float32)).item()\n",
    "\n",
    "                m_arr = np.array([M[i - 1, j - 1] + sim, U[i, j], L[i, j]])\n",
    "\n",
    "            M[i, j] = np.max(m_arr)\n",
    "            m_where = m_arr == np.max(m_arr)\n",
    "\n",
    "            idx = np.hstack([m_where, u_where, l_where])\n",
    "            tracer[i, j, idx] = 1\n",
    "\n",
    "\n",
    "    alignment = []\n",
    "    alignment.append(traceback_sg(tracer, seq_1, seq_2, mat=M, affine=True,\n",
    "                                  local= True, roadmap=0))\n",
    "    alignment = list(set(map(tuple, alignment)))\n",
    "\n",
    "    return M, L, U, tracer, alignment\n",
    "\n",
    "\n",
    "def traceback_sg(tracer, seq_1, seq_2, mat=None, local=False, affine=False, roadmap=0):\n",
    "\n",
    "    m = len(seq_1);\n",
    "    n = len(seq_2)\n",
    "\n",
    "    x = np.array(list(seq_1), dtype='object')\n",
    "    y = np.array(list(seq_2), dtype='object')\n",
    "\n",
    "    # set start location\n",
    "    if roadmap == 0:\n",
    "        r = np.random.choice(range(np.size(np.where(mat == np.max(mat))[0])), 1)[0]  # random maxima\n",
    "    elif roadmap == 1:\n",
    "        r = -1\n",
    "    elif roadmap == 2:\n",
    "        r = 0\n",
    "\n",
    "    st = [(np.where(mat == np.max(mat))[0][r]) + 1, (np.where(mat == np.max(mat))[1][r]) + 1]\n",
    "\n",
    "    # set starting gaps based on the start location\n",
    "    start_size = ((m - st[0]) - (n - st[1]))  # how many gaps and for which sequence\n",
    "    start_gap = (['-'] * abs(start_size))\n",
    "    if start_size > 0:\n",
    "        y = np.append(y, start_gap)\n",
    "    elif start_size < 0:\n",
    "        x = np.append(x, start_gap)\n",
    "\n",
    "    st_lv = 0  # start in midgard\n",
    "\n",
    "    while ((st[0] > 1) & (st[1] > 1)):\n",
    "\n",
    "        B = np.zeros([2, 2])  # define 2x2 box which specifies which way to move\n",
    "\n",
    "        if affine is True:\n",
    "            Tr = np.zeros([7])  # define a 7x1 Tr array (will store arrows at each step)\n",
    "        else:\n",
    "            Tr = np.zeros([3])  # define a 3x1 Tr array (will store arrows at each step)\n",
    "\n",
    "\n",
    "        if affine is False:\n",
    "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
    "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
    "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
    "\n",
    "        else:\n",
    "            # tracer\n",
    "            Tr[0] = np.copy(tracer[st[0] - 1, st[1] - 1, 0])\n",
    "            Tr[1] = np.copy(tracer[st[0] - 1, st[1] - 1, 1])\n",
    "            Tr[2] = np.copy(tracer[st[0] - 1, st[1] - 1, 2])\n",
    "            Tr[3] = np.copy(tracer[st[0] - 1, st[1] - 1, 3])\n",
    "            Tr[4] = np.copy(tracer[st[0] - 1, st[1] - 1, 4])\n",
    "            Tr[5] = np.copy(tracer[st[0] - 1, st[1] - 1, 5])\n",
    "            Tr[6] = np.copy(tracer[st[0] - 1, st[1] - 1, 6])\n",
    "\n",
    "        # bifurcations\n",
    "        if affine is True:\n",
    "            levels = [[2, 0, 1], [4, 3], [6, 5]]\n",
    "        else:\n",
    "            levels = [[2, 0, 1]]\n",
    "        for l in levels:\n",
    "            if np.sum(Tr[l]) > 1:\n",
    "                choose = np.where(Tr[l] == 1)[0]\n",
    "                Tr[l] = 0\n",
    "                if roadmap == 0:\n",
    "                    r = np.random.choice(choose, 1)[0]  # random turning\n",
    "                elif roadmap == 1:\n",
    "                    r = choose[-1]  # highroad\n",
    "                elif roadmap == 2:\n",
    "                    r = choose[0]  # lowroad\n",
    "                else:\n",
    "                    raise Exception(\"roadmap only accepts 0: random turning, 1: highroad, 2: lowroad\")\n",
    "                Tr[l[r]] = 1\n",
    "\n",
    "        # level up-down\n",
    "        if ((Tr[0] == 1) & (st_lv == 0)):  # diagonal\n",
    "            B[0, 0] = 1\n",
    "\n",
    "        if ((Tr[1] == 1) & (st_lv == 0)):\n",
    "            if affine is True:\n",
    "                st_lv = 1  # level up\n",
    "            else:\n",
    "                B[0, 1] = 1\n",
    "\n",
    "        if ((Tr[2] == 1) & (st_lv == 0)):\n",
    "            if affine is True:\n",
    "                st_lv = 2  # level down\n",
    "            else:\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        # affine gaps allow for level shifts\n",
    "        if affine is True:\n",
    "            if ((Tr[4] == 1) & (st_lv == 1)):  # move up\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if ((Tr[3] == 1) & (st_lv == 1)):  # move up back to main\n",
    "                st_lv = 0\n",
    "                B[0, 1] = 1\n",
    "\n",
    "            if ((Tr[6] == 1) & (st_lv == 2)):  # move left\n",
    "                B[1, 0] = 1\n",
    "\n",
    "            if ((Tr[5] == 1) & (st_lv == 2)):  # move left back to main\n",
    "                st_lv = 0\n",
    "                B[1, 0] = 1\n",
    "\n",
    "        if local is True:\n",
    "            if (mat[st[0] - 1, st[1] - 1] == 0):\n",
    "                break\n",
    "\n",
    "        # movements\n",
    "        if B[0, 1] == 1:  # upward\n",
    "            y = np.insert(y, st[1] - 1, '-')  # add a gap\n",
    "            st[0] = st[0] - 1\n",
    "\n",
    "        if B[1, 0] == 1:  # leftward\n",
    "            x = np.insert(x, st[0] - 1, '-')  # add a gap\n",
    "            st[1] = st[1] - 1\n",
    "\n",
    "        if B[0, 0] == 1:  # diagonal\n",
    "            st[1] = st[1] - 1\n",
    "            st[0] = st[0] - 1\n",
    "\n",
    "    # some end gaps are left when you hit the upper/lower end of the matrix or a 0\n",
    "    end_size = (np.size(x) - np.size(y))  # how many gaps and for which sequence\n",
    "    end_gap = (['-'] * abs(end_size))\n",
    "    if end_size > 0:\n",
    "        y = np.insert(y, 0, end_gap)\n",
    "    elif end_size < 0:\n",
    "        x = np.insert(x, 0, end_gap)\n",
    "\n",
    "    # check no overlapping gaps\n",
    "    x = np.where(((x == '-') & (y == '-')), None, x)\n",
    "    y = np.where((x == None), '', y)\n",
    "    x = np.where((x == None), '', x)\n",
    "\n",
    "    return np.sum(x), np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IMkkTPWsaF0"
   },
   "source": [
    "# Aux Funx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LBNOVyaN7ZT3"
   },
   "outputs": [],
   "source": [
    "def load_fasta(path):\n",
    "    \"\"\" Loads the two sequences from the FASTA file \"\"\"\n",
    "    fasta_sequences = SeqIO.parse(open(path),'fasta')\n",
    "    \n",
    "    # Array of (name, sequence) tuples\n",
    "    sequences = []\n",
    "\n",
    "    for fasta in fasta_sequences:\n",
    "        name, sequence = fasta.id, str(fasta.seq)\n",
    "        sequences.append((name, sequence.upper()))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "def just_seqs(seqs):\n",
    "    \"\"\" Extracts the sequence from a list of (name, sequence) tuples \"\"\"\n",
    "    final_seqs = []\n",
    "    for seq in seqs:\n",
    "        final_seqs.append(seq[1])\n",
    "\n",
    "    return final_seqs\n",
    "\n",
    "def aligned_to_indexed(seqs):\n",
    "  \"\"\" Removes dashes (-) in a sequence and creates positions array for non-dash residues \"\"\"\n",
    "  no_dash = []\n",
    "  positions = []\n",
    "  for seq in seqs:\n",
    "    no_dash.append(seq.replace(\"-\" , \"\"))\n",
    "    pos = []\n",
    "    for i , char in enumerate(seq):\n",
    "      if char != \"-\":\n",
    "        pos.append(i)\n",
    "    positions.append(pos)\n",
    "\n",
    "  return no_dash, positions\n",
    "\n",
    "def length_matcher(x , y , place = \"\"):\n",
    "  \"\"\" Matches the length between x and y with spaces if necessary \"\"\"\n",
    "  length = 5\n",
    "\n",
    "  if len(x) < length:\n",
    "    spaces = abs(len(x) - length)\n",
    "\n",
    "    if place == \"Back\":\n",
    "      x = \" \" * spaces + x\n",
    "    if place == \"Front\":\n",
    "      x = x + \" \" * spaces\n",
    "\n",
    "  if len(y) < length:\n",
    "    spaces = abs(len(y) - length)\n",
    "\n",
    "    if place == \"Back\":\n",
    "      y = \" \" * spaces + y\n",
    "    if place == \"Front\":\n",
    "      y = y + \" \" * spaces\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "achF78kUscys"
   },
   "source": [
    "# Alignment Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DawofHCVAdVo"
   },
   "outputs": [],
   "source": [
    "def get_alignments(prot1, prot2, gap_penalty = 0, gap_extension_penalty = 0 ,\n",
    "                   scoring = \"ProtT5\" , alignment_type = \"Global-regular\" , Model = \"\" , Model_Tokenizer = \"\"):\n",
    "    \"\"\" Gets the alignments between two sequences \"\"\"\n",
    "\n",
    "    if alignment_type == \"Global-regular\":\n",
    "      M, L, U , tracer , alignment = affine_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty\n",
    "                                                    ,scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
    "      max_score = np.max(M)\n",
    "\n",
    "    if alignment_type == \"Global-end-gap-free\" or alignment_type == \"End-Gap-Free\":\n",
    "      M, L, U , tracer , alignment = affine_semi_global_dp(prot1, prot2, gap_penalty, gap_extension_penalty\n",
    "                                                    ,scoring = scoring , Model = Model, Model_tokenizer = Model_Tokenizer)\n",
    "      max_score = max(M[-1,-1],L[-1,-1],U[-1,-1])\n",
    "\n",
    "    # Return (reference alignment, query alignment, alignment score)\n",
    "    aligned1 = alignment[0][0]\n",
    "    aligned2 = alignment[0][1]\n",
    "\n",
    "    return aligned1, aligned2, max_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5Hfj5rAk4_cg"
   },
   "outputs": [],
   "source": [
    "def get_visualization(prot1, prot2 , score , Type = \"\" , Model = \"\" , Model_Tokenizer = \"\"):\n",
    "  MODELS_LIST = [\"ProtT5\" , \"ProtBert\" , \"ProtAlbert\" , \"ProtXLNet\" , \"ESM1b\" , \"ESM2\"]\n",
    "  cos = torch.nn.CosineSimilarity(dim=0)\n",
    "\n",
    "  seqs = [prot1 , prot2]\n",
    "  no_dash , positions = aligned_to_indexed(seqs)\n",
    "  model = Model\n",
    "  tokenizer = Model_Tokenizer\n",
    "\n",
    "  # Get embeddings\n",
    "  if Type == \"ProtT5\":\n",
    "    p1_emb = get_embs_T5(model, tokenizer, [no_dash[0]] , 1)\n",
    "    p2_emb = get_embs_T5(model, tokenizer, [no_dash[1]] , 1)\n",
    "    \n",
    "  elif Type == \"ProtBert\":\n",
    "    p1_emb = get_embs_ProtBert(model, tokenizer, [no_dash[0]] , 1)\n",
    "    p2_emb = get_embs_ProtBert(model, tokenizer, [no_dash[1]] , 1)\n",
    "    \n",
    "  elif Type == \"ProtAlbert\":\n",
    "    p1_emb = get_embs_ProtAlbert(model, tokenizer, [no_dash[0]] , 1)\n",
    "    p2_emb = get_embs_ProtAlbert(model, tokenizer, [no_dash[1]] , 1)\n",
    "    \n",
    "  elif Type == \"ProtXLNet\":\n",
    "    p1_emb = get_embs_XLNet(model, tokenizer, [no_dash[0]] , 1)\n",
    "    p2_emb = get_embs_XLNet(model, tokenizer, [no_dash[1]] , 1)\n",
    "    \n",
    "  elif Type == \"ESM1b\":\n",
    "    p1_emb = get_embs_ESM1b(model, tokenizer, [no_dash[0]] , 1)\n",
    "    p2_emb = get_embs_ESM1b(model, tokenizer, [no_dash[1]] , 1)\n",
    "    \n",
    "  elif Type == \"ESM2\":\n",
    "    p1_emb = get_embs_ESM2(model, tokenizer, [no_dash[0]] , 1)\n",
    "    p2_emb = get_embs_ESM2(model, tokenizer, [no_dash[1]] , 1)\n",
    "\n",
    "  p1_revived = \"\"\n",
    "  p2_revived = \"\"\n",
    "  aligned_info = \"\"\n",
    "\n",
    "  for i in range(len(prot1)):\n",
    "\n",
    "    if i in positions[0]:\n",
    "      p1_revived += prot1[i]\n",
    "    else:\n",
    "      p1_revived += \"-\"\n",
    "\n",
    "    if i in positions[1]:\n",
    "      p2_revived += prot2[i]\n",
    "    else:\n",
    "      p2_revived += \"-\"\n",
    "\n",
    "\n",
    "    if p1_revived[-1] == p2_revived[-1]:\n",
    "      aligned_info += p1_revived[-1]\n",
    "\n",
    "    elif p1_revived[-1] == \"-\" or p2_revived[-1] == \"-\":\n",
    "      aligned_info += \" \"\n",
    "\n",
    "    elif p1_revived[-1] != p2_revived[-1]:\n",
    "\n",
    "      if Type in MODELS_LIST:\n",
    "        sim = cos(torch.tensor(p1_emb[0][positions[0].index(i)] , dtype = torch.float32) ,\n",
    "                  torch.tensor(p2_emb[0][positions[1].index(i)] , dtype = torch.float32)).item()\n",
    "\n",
    "        aligned_info += \" \"\n",
    "\n",
    "  del model\n",
    "  del tokenizer\n",
    "\n",
    "  return p1_revived , aligned_info, p2_revived, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7nB5_MdnBO5"
   },
   "source": [
    "# Alignment For 2 Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dyAQgxCFn99V"
   },
   "outputs": [],
   "source": [
    "def alignment_file_TXT(saving_add, seqs_path, scoring, alignment_type,\n",
    "                      gap_penalty, gap_extension_penalty):\n",
    "  \"\"\" Creates and outputs the alignmment file between two sequences \"\"\"\n",
    "  print(f\"Device: {device}\")\n",
    "\n",
    "  # Get selected model\n",
    "  if scoring == \"ProtT5\":\n",
    "    Model , Model_Tokenizer = ProtT5_initialize()\n",
    "  elif scoring == \"ProtBert\":\n",
    "    Model , Model_Tokenizer = ProtBert_initialize()\n",
    "  elif scoring ==  \"ProtAlbert\":\n",
    "    Model , Model_Tokenizer = ProtAlbert_initialize()\n",
    "  elif scoring ==  \"ProtXLNet\":\n",
    "    Model , Model_Tokenizer = XLNet_initialize()\n",
    "  elif scoring ==  \"ESM1b\":\n",
    "    Model , Model_Tokenizer = ESM1b_initialize()\n",
    "  elif scoring ==  \"ESM2\":\n",
    "    Model , Model_Tokenizer = ESM2_initialize()\n",
    "\n",
    "  # Load sequences from FASTA file\n",
    "  seqs = load_fasta(seqs_path)\n",
    "\n",
    "  # Get protein sequences\n",
    "  prot1 = seqs[0][1]\n",
    "  prot2 = seqs[1][1]\n",
    "\n",
    "  # Get names of protein sequences\n",
    "  name1 = seqs[0][0]\n",
    "  name2 = seqs[1][0]\n",
    "\n",
    "  # Get alignments and visualization\n",
    "  reference_al, query_al, al_score = get_alignments(prot1, prot2, gap_penalty = gap_penalty,\n",
    "                    gap_extension_penalty = gap_extension_penalty ,\n",
    "                                              scoring = scoring , alignment_type = alignment_type,\n",
    "                                              Model = Model , Model_Tokenizer = Model_Tokenizer)\n",
    "\n",
    "  p1_al , aligned_info , p2_al , al_score = get_visualization(reference_al , query_al, al_score , Type = scoring,\n",
    "                                                              Model = Model, Model_Tokenizer = Model_Tokenizer)\n",
    "\n",
    "  full_al_1 = p1_al\n",
    "  full_al_2 = p2_al\n",
    "\n",
    "  if not os.path.exists(saving_add):\n",
    "   os.makedirs(saving_add)\n",
    "\n",
    "  file_name = saving_add + seqs_path.split(\"/\")[-1].split(\".\")[-2] + \"_\" + scoring + \"_\" + alignment_type + \"_\"\n",
    "  file_name += str(gap_penalty) + \"_\" + str(gap_extension_penalty) + \"_\"+ \"Alignment\" + \".txt\"\n",
    "  f = open(file_name, \"w\")\n",
    "\n",
    "  # Write Sequence 1 Information\n",
    "  f.write(\"Seq 1 \\n\")\n",
    "  f.write(\">\" + name1 + \"\\n\")\n",
    "  f.write(reference_al.replace(\"-\" , \"\") + \"\\n\")\n",
    "    \n",
    "  # Write Sequence 2 Information\n",
    "  f.write(\"Seq 2 \\n\")\n",
    "  f.write(\">\" + name2 + \"\\n\")\n",
    "  f.write(query_al.replace(\"-\" , \"\") + \"\\n\\n\")\n",
    "    \n",
    "  # Write Alignment Information\n",
    "  f.write(\"Alignment Type : \" + alignment_type + \"\\n\\n\")\n",
    "  f.write(\"Opening Gap Penalty : \" + str(gap_penalty) + \"\\n\")\n",
    "  f.write(\"Extension Gap Penalty : \" + str(gap_extension_penalty) + \"\\n\")\n",
    "  f.write(\"Scoring System : \" + scoring + \"\\n\")\n",
    "  f.write(\"Score : \"  + str(al_score) + \"\\n\\n\")\n",
    "\n",
    "  p1_pos = 1\n",
    "  p2_pos = 1\n",
    "  aligned_gaps = \"\"\n",
    "\n",
    "  for j in range(int(len(p1_al) / 60) + 1):\n",
    "    p1_posix = p1_al[j * 60: (j + 1) * 60]\n",
    "    p2_posix = p2_al[j * 60: (j + 1) * 60]\n",
    "    p1_back_str, p2_back_str = length_matcher(str(p1_pos) , str(p2_pos) , place = \"Front\")\n",
    "\n",
    "    for k in range(len(p1_posix)):\n",
    "      if p1_posix[k] != \"-\":\n",
    "        p1_pos += 1\n",
    "      if p2_posix[k] != \"-\":\n",
    "        p2_pos += 1\n",
    "\n",
    "    p1_end_str, p2_end_str = length_matcher(str(p1_pos - 1) , str(p2_pos - 1) , place = \"Back\")\n",
    "    aligned_gaps = \" \" * len(p1_back_str)\n",
    "\n",
    "    f.write(\"Seq 1 : \" + p1_back_str + \" \" + p1_al[j * 60: (j + 1) * 60] + \" \" + p1_end_str + \"\\n\")\n",
    "    f.write(\"        \"  +  aligned_gaps + \" \" + aligned_info[j * 60: (j + 1) * 60] + \"\\n\")\n",
    "    f.write(\"Seq 2 : \"  + p2_back_str + \" \" + p2_al[j * 60: (j + 1) * 60] + \" \" + p2_end_str + \"\\n\\n\")\n",
    "\n",
    "  print(\"Alignment Computation is Done!\")\n",
    "  del Model\n",
    "  del Model_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OaiQRMPjkWbg"
   },
   "outputs": [],
   "source": [
    "def user_guide(MODELS_LIST , ALIGNMENT_TYPES):\n",
    "  \"\"\" User guide for the E-score program \"\"\"\n",
    "  print(\"Parameters & Descriptions:\\n\")\n",
    "    \n",
    "  print(\"saving_add: output directory path\")\n",
    "  print(\"seqs_path: path to the FASTA file with two protein sequences\")\n",
    "    \n",
    "  print(\"scoring_type: model for embedding production (\", end = \"\")\n",
    "  for model_name in MODELS_LIST[:-1] : print(model_name + \", \" , end = \"\")\n",
    "  print(MODELS_LIST[-1] + \")\")\n",
    "\n",
    "  print(\"alignment_type: Global-regular or Global-end-gap-free\")\n",
    "  print(\"gap_penalty: -1 (default); Recommended Values: -4, -3, -2, -1.5, -1, -0.5\")\n",
    "  print(\"gap_extension_penalty: -0.2 (default); Recommended Values: -1, -0.8, -0.5, -0.3, -0.2, -0.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVRUmBYkG4M3"
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Tyat_ydz5f4",
    "outputId": "d0b958ab-ed49-469e-ae38-44c0c744c10a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters & Descriptions:\n",
      "\n",
      "saving_add: output directory path\n",
      "seqs_path: path to the FASTA file with two protein sequences\n",
      "scoring_type: model for embedding production (ProtT5, ESM2, ProtBert, ProtAlbert, ESM1b, ProtXLNet)\n",
      "alignment_type: Global-regular or Global-end-gap-free\n",
      "gap_penalty: -1 (default); Recommended Values: -4, -3, -2, -1.5, -1, -0.5\n",
      "gap_extension_penalty: -0.2 (default); Recommended Values: -1, -0.8, -0.5, -0.3, -0.2, -0.1\n"
     ]
    }
   ],
   "source": [
    "user_guide(MODELS_LIST , ALIGNMENT_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glng92tRG5ZZ",
    "outputId": "f876fd26-98b1-4520-c2ed-337e2758f0ff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "ProtT5 Initialize: \n",
      "Loading T5 Model: Rostlab/prot_t5_xl_half_uniref50-enc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading T5 Tokenizer\n",
      "Generating T5 Embeddings\n",
      "Generating T5 Embeddings\n",
      "Generating T5 Embeddings\n",
      "Generating T5 Embeddings\n",
      "Alignment Computation is Done!\n"
     ]
    }
   ],
   "source": [
    "# Defining Parameters\n",
    "saving_add =  \"./content/\"\n",
    "seqs_path = \"tests/Test2.fasta\"\n",
    "scoring = MODELS_LIST[0]\n",
    "alignment_type = ALIGNMENT_TYPES[0]\n",
    "gap_penalty = -1\n",
    "gap_extension_penalty = -0.2\n",
    "\n",
    "# Generating Alignment File\n",
    "alignment_file_TXT(saving_add = saving_add , seqs_path = seqs_path, scoring = scoring, alignment_type = alignment_type,\n",
    "                      gap_penalty = gap_penalty, gap_extension_penalty = gap_extension_penalty)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Dq9u86RyHKk_",
    "8LCdgRFBFWW6",
    "8ctdjBhTGi9z",
    "wiNZQ2SQJM54",
    "9IMkkTPWsaF0"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
