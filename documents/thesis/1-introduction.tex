\chapter{Introduction}
Proteins are one of the four molecules of life. Finding similarities among protein sequences is essential in identifying protein structure and function. This is done by computing alignments between sequences.

The \textit{E}-score method is a method computes alignments between sequences using contextual embeddings produced by \gls{transformer} models (\cite{Ashrafzadeh:2023}). This method uses several different transformer models based off of \gls{NLP} models (\cite{Devlin:2018, Zhenzhong:2020, Liu:2019, Yang:2022, Raffel:2020}).

These transformer models produce embeddings when provided protein sequences. Understanding the values and distributions of these embeddings between each model is one focus of this research (\hyperlink{O1}{O1}).

\textit{E}-score uses cosine similarity to compute similarity between pairs of embeddings for scoring alignments. This research analyzes the distributions of observed cosine similarity results for natural and random protein sequences (\hyperlink{O2}{O2}, \hyperlink{O3}{O3}).

Combining embedding distribution and cosine similarity results with biochemical understandings of proteins is used to draw conclusions about model performance and \textit{E}-score results. Specifically, explanations about why some models outperform other models are derived (\hyperlink{O1}{O1}, \hyperlink{O2}{O2}).

Using inference about the proposed factors contributing to \textit{E}-score performance, I describe the procedure and techniques for fine-tuning ProtT5 to produce better embeddings for the \textit{E}-score method (\hyperlink{O4}{O4}).

\noindent Significant results from this research include:
\begin{itemize}
    \item{Significant positive correlation between higher embedding value variance and improved \textit{E}-score performance for a given model.}
    \item{Significant positive correlation between average cosine similarity results approaching 0 and improved \textit{E}-score performance for a given model.}
\end{itemize}

Novel implications about model flexibility and fine-tuning models to better adapt to the frequency of \glspl{residue} (or words in \gls{NLP}) provide significant insight into improving performance of different models for not only \textit{E}-score, but for any method using \glspl{transformer}.

\section{Report structure}
Chapter 2 provides a reader with background on important concepts and details discussed later in the thesis. Chapter 3 outlines the objectives of the research. Chapter 4 outlines the materials and methods used in the research conducted on the \textit{E}-score method. Chapter 5 provides the results from analysis performed in the data science investigation. Chapter 6 discusses the results, their implications, limitations, and generalizations. Chapter 7 concludes the study by addressing the research questions outlined in the thesis proposal, and discusses impact and novelty of the results. Chapter 8 discusses potential future work and novel lessons learned from this research.