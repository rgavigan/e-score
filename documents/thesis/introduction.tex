%======================================================================
\chapter{Introduction}
%======================================================================

Proteins are one of the four molecules of life. Finding similarities among protein sequences is essential in identifying protein structure and function. This is done by computing alignments between sequences.

The \textit{E}-score method is a method to compute alignments between sequences using contextual embeddings produced by \gls{transformer} models \cite{Ashrafzadeh:2023}. This method uses several different transformer models based off of models in \gls{NLP}.

This research addresses the results observed for the \textit{E}-score method. Namely, I explain the observed cosine similarity results and explain significant differences and similarities between the models used (Table \ref{tab:transformers}), both qualitative and quantitative. Combining the comparison of models with visualization and analysis of embedding vector and cosine similarity distributions, I propose the contributing factors to better \textit{E}-score performance.

Using inference about the proposed factors contributing to \textit{E}-score performance, I describe the procedure and techniques for fine-tuning ProtT5 and other models to produce better embeddings for sequence alignment.

\section{Thesis outline}
Chapter 2 provides a reader with background on important concepts and details discussed later in the thesis. Chapter 3 outlines the materials and methods used in the research conducted on the \textit{E}-score method. Chapter 4 provides the results from analysis performed in the data science investigation. Chapter 5 concludes the study by addressing the research questions outlined in the thesis proposal, and discusses impact and novelty of the results.