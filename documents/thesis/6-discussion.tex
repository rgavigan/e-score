\chapter{Discussion}

\section{Implications}
The results derive implications that can be used to enhance the \textit{E}-score method with \gls{LoRA} fine-tuning. This is primarily applicable to ProtT5, as it is the best-performing method with pre-made fine-tuning notebooks that can expanded upon, but is also applicable to other models. We can fine-tune the other models to catch up to ProtT5's performance by generating embeddings with more variance in their average values. Additionally, we can create a custom penalty function to punish these models for producing mostly similar cosine similarity results, bringing the average cosine similarity result closer to 0 and closer to ProtT5's average.

The connection between observations in nature and embedding results from models is highly evident from this research. Most models fail to capture variance because of these laws governing nature, which ProtT5 managed to overcome (likely only because of its size) as results in Section 5.4 outlined. AlphaFold (\cite{Jumper:2021}) is a protein structure prediction method developed by Google DeepMind that uses protein transformers as the E-score method does. Because we are able to predict protein structure with transformers, it is evident that primary structures, secondary structures, structural motifs, and other properties of proteins are heavily correlated. Results from Section 5.1 and 5.2 further support this claim and outline the importance of adapting models to account for these rules. More efficient training strategies can be researched to improve the performance of models despite identical size and training time (more compute-optimal).

\section{Limitations and Generalizations}
Limited compute power (GPU: RTX 4070 Super) impacted the scale of embedding distribution and cosine similarity distribution procedures. With more compute power, these procedures could be conducted for all 49 \glspl{MSA} used in the \textit{E}-score method and an equivalent number of random sequences. This would greatly improve the validity of the results.

% Generalizability
Results are generalizable to other systems utilizing ProtTrans, ESM-1b, and ESM2 models (\cite{Elnaggar:2021, Rives:2021}). Novel conclusions can be used to support fine-tuning models for their respective use-cases. \gls{NLP} use cases may repeat experimental procedures in future research to determine word frequency (ex: the word "what" is much more common in English than "myriad"), embedding distribution (\hyperlink{O1}{O1}), and their correlation with the results of a respective method (such as \textit{E}-score's \hyperlink{O3}{O3}) to find parallel conclusions.